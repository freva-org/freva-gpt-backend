model_list:
  - model_name: "gpt-4.1"
    litellm_params:
      model: "openai/gpt-4.1"
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      supports_function_calling: true

  - model_name: "gpt-4.1-mini"
    litellm_params:
      model: "openai/gpt-4.1-mini"
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      supports_function_calling: true

  - model_name: "gpt-4.1-nano"
    litellm_params:
      model: "openai/gpt-4.1-nano"
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      supports_function_calling: true

  - model_name: "gpt-5-pro"
    litellm_params:
      model: "openai/gpt-5-pro"
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      supports_function_calling: true

  - model_name: "gpt-5"
    litellm_params:
      model: "openai/gpt-5"
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      supports_function_calling: true

  - model_name: "gpt-5-mini"
    litellm_params:
      model: "openai/gpt-5-mini"
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      supports_function_calling: true

  - model_name: "gpt-5-nano"
    litellm_params:
      model: "openai/gpt-5-nano"
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      supports_function_calling: true

  - model_name: "o3"
    litellm_params:
      model: "openai/o3"
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      supports_function_calling: true

  - model_name: "o4-mini"
    litellm_params:
      model: "openai/o4-mini"
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      supports_function_calling: true

  - model_name: "gpt-4o"
    litellm_params:
      model: "openai/gpt-4o"
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      supports_function_calling: true

  - model_name: "gpt-4o-mini"
    litellm_params:
      model: "openai/gpt-4o-mini"
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      supports_function_calling: true

  - model_name: "llama3.1-disabled"
    litellm_params:
      model: "openai/llama3.1"
      api_base: "http://ollama:11434/v1"
    model_info:
      supports_function_calling: true
      #  - model_name: "deepseek-r1" # Doesn't do tool calls yet :/
  #    litellm_params:
  #      model: "openai/deepseek-r1"
  #      api_base: "http://ollama:11434/v1"
  #    model_info:
  #      supports_function_calling: true

  #- model_name: "qwen3:235b-a22b-instruct-2507-q4_K_M" # Doesn't fit into VRAM, ollama doesn't do VRAM->RAM offloading
  # litellm_params:
  # model: "openai/qwen3:235b-a22b-instruct-2507-q4_K_M"
  #api_base: "http://ollama:11434/v1"
  #model_info:
  #supports_function_calling: true

  - model_name: "gpt-oss:20b" # Open Source model from Openai
    litellm_params:
      model: "openai/gpt-oss:20b"
      api_base: "http://ollama:11434/v1"
    model_info:
      supports_function_calling: true

  # - model_name: "qwen3-coder:30b-TODO" # Doesn't support tool calling from Ollama yet
  #   litellm_params:
  #     model: "openai/qwen3-coder:30b"
  #     api_base: "http://ollama:11434/v1"
  #   model_info:
  #     supports_function_calling: true

  - model_name: "qwen3:30b-a3b"
    litellm_params:
      model: "openai/qwen3:30b"
      api_base: "http://ollama:11434/v1"
    model_info:
      supports_function_calling: true

  - model_name: "qwen3:32b"
    litellm_params:
      model: "openai/qwen3:32b"
      api_base: "http://ollama:11434/v1"
    model_info:
      supports_function_calling: true

  - model_name: "llama4:scout-disabled"
    litellm_params:
      model: "openai/llama4:scout"
      api_base: "http://ollama:11434/v1"
    model_info:
      supports_function_calling: true

      # Development models
  - model_name: "qwen3:4b"
    litellm_params:
      model: "openai/qwen3:4b"
      api_base: "http://host.docker.internal:11434/v1" # For development only!
    model_info:
      supports_function_calling: true
  - model_name: "qwen2.5:3b"
    litellm_params:
      model: "openai/qwen2.5:3b"
      api_base: "http://host.docker.internal:11434/v1" # For development only!
    model_info:
      supports_function_calling: true

litellm_settings:
  drop_params: true # Not all models support the same parameters. LiteLLM can automatically drop unsupported parameters.
  # set_verbose: true # For debugging, prints a lot more info, including secrets. Leave this on during production is highly discouraged.
