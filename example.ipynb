{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking whether the API is up\n",
    "The `/ping` endpoint (`/help` is equivalent) is meant to give an overview over whether the API is up, what version it is running and what state it has. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"endpoints\": [\n",
      "    {\n",
      "      \"methods\": [\n",
      "        \"get\"\n",
      "      ],\n",
      "      \"name\": \"ping\",\n",
      "      \"params\": {},\n",
      "      \"return_type\": \"json{version:string,streamvariants:list{string},endpoints:list{name:string,methods:string,params:list{json},returntype:json}}\"\n",
      "    },\n",
      "    {\n",
      "      \"methods\": [\n",
      "        \"get\"\n",
      "      ],\n",
      "      \"name\": \"docs\",\n",
      "      \"params\": {},\n",
      "      \"return_type\": \"string\"\n",
      "    },\n",
      "    {\n",
      "      \"methods\": [\n",
      "        \"get\"\n",
      "      ],\n",
      "      \"name\": \"getthread\",\n",
      "      \"params\": {\n",
      "        \"auth_key\": \"string\",\n",
      "        \"thread_id\": \"string\"\n",
      "      },\n",
      "      \"return_type\": \"json{list{variant:streamvariant=string,content:string}}\"\n",
      "    },\n",
      "    {\n",
      "      \"methods\": [\n",
      "        \"get\"\n",
      "      ],\n",
      "      \"name\": \"streamresponse\",\n",
      "      \"params\": {\n",
      "        \"auth_key\": \"string\",\n",
      "        \"input\": \"string\",\n",
      "        \"thread_id\": \"optional{string}\"\n",
      "      },\n",
      "      \"return_type\": \"stream{json{variant:streamvariant=string,content:string}}\"\n",
      "    },\n",
      "    {\n",
      "      \"methods\": [\n",
      "        \"get\",\n",
      "        \"post\"\n",
      "      ],\n",
      "      \"name\": \"stop\",\n",
      "      \"params\": {\n",
      "        \"auth_key\": \"string\",\n",
      "        \"thread_id\": \"string\"\n",
      "      },\n",
      "      \"return_type\": \"\"\n",
      "    }\n",
      "  ],\n",
      "  \"streamvariants\": [\n",
      "    \"Prompt\",\n",
      "    \"User\",\n",
      "    \"Assistant\",\n",
      "    \"Code\",\n",
      "    \"CodeOutput\",\n",
      "    \"Image\",\n",
      "    \"ServerError\",\n",
      "    \"OpenAIError\",\n",
      "    \"CodeError\",\n",
      "    \"StreamEnd\",\n",
      "    \"ServerHint\"\n",
      "  ],\n",
      "  \"version\": \"1.5.1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example use of the /ping endpoint\n",
    "import requests\n",
    "\n",
    "base_url = 'http://localhost:8502/api/chatbot' # Change the URL to vader5 if you are running it against vader5.\n",
    "url = base_url + '/ping'\n",
    "response = requests.get(url)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ping response\n",
    "The ping response is largely for manual checking, but can also be used to verify that the interface to the server is as expected. \n",
    "A ping response is a JSON Object that contains the version, a list of Stream Variants and all endpoints.\n",
    "The version follows [semver](https://semver.org/), \n",
    "\n",
    "As can be seen in the evaluated code above, the ping response describes itself:\n",
    "It writes its own version as a string, the endpoints in a list where each endpoint describes its name as string, methods as a list of strings, its return_type as a String and its params as a JSON map. \n",
    "\n",
    "It also gives a list of all StreamVariants the server supports. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream variants\n",
    "To give the client as much information about the conversation with the LLM as necessary, seperate Stream variants are used to mark different events or participants of the conversation. \n",
    "\n",
    "As of version 1.4.1, there are 11 Stream variants: Prompt,User,Assistant,Code,CodeOutput,Image,ServerError,OpenAIError,CodeError,StreamEnd,ServerHint; as can also be read when running `/docs`. \n",
    "Changing the behaviour of variants requires a major version bump and adding variants requires a minor version bump in accordance to semver. \n",
    "This mean that as long as the major version of the backend matches the major version the client was written for, the client can expect its handling of the stream variants to be correct (or suffient, as it can ignore new variants).\n",
    "All unknown Stream variants can be safely ignored by the client. \n",
    "\n",
    "These are the current Stream variants in Version 1.4.*:\n",
    "- Prompt: The prompt the LLM got when it started the conversation. Mainly for debugging; should not be shown to the user outside of debugging. \n",
    "- User: The input of the user, as a String\n",
    "- Assistant: The output of the Assistant, as a String. Often Markdown, because the LLM can output Markdown.\n",
    "Multiple messages of this variant after each other belong to the same message, but are broken up due to the stream.\n",
    "- Code: The code that the Assistant generated, as a String. It will be executed on the backend.\n",
    "Currently, only Python is supported. The content is not formatted.\n",
    "- CodeOutput: The output of the code that was executed, as a String. Also not formatted.\n",
    "- Image: An image that was generated during the conversation, as a String. The image is Base64 encoded.\n",
    "An example of this would be a matplotlib plot.\n",
    "- ServerError: An error that occured on the server(backend) side, as a String. Contains the error message.\n",
    "The client should realize that this error occured and handle it accordingly; most ServerErrors are immeadiately followed by a StreamEnd.\n",
    "- OpenAI Error: An error that occured on the OpenAI side, as a String. Contains the error message.\n",
    "These are often for the rate limits, but can also be for other things, i.E. if the API is down.\n",
    "- CodeError: The Code from the LLM could not be executed or there was some other error while setting up the code execution.\n",
    "- StreamEnd: The Stream ended. Contains a reason as a String. This is always the last message of a stream.\n",
    "If the last message is not a StreamEnd but the stream ended, it's an error from the server side and needs to be fixed.\n",
    "- ServerHint: The Server hints something to the client. This is primarily used for giving the thread_id.\n",
    "The Content is in JSON format, with the key being the hint and the value being the content. Currently, only the keys \"thread_id\" and \"warning\" are used.\n",
    "An example for a ServerHint packet would be `{\"variant\": \"ServerHint\", \"content\": \"{\\\"thread_id\\\":\\\"1234\\\"}\"}`.\n",
    "That means that the content needs to be parsed as JSON to get the actual content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "To make sure that not everyone on the network can simply use the API, a base authentication is in place that requires the client to send, with every request, an auth_key that has to match the one on the server. \n",
    "\n",
    "Because this doesn't help much regarding overall safety, a move to oauth2 is being worked on. Then, the frontend must simply authenticate with their freva token and the username is automatically retrieved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## /ping\n",
    "`/ping` and `/help` are meant to allow the client to check whether the server is up and to get the version as well as a quick overview over the API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## /docs\n",
    "`/docs` is supposed to only be used manually to check how to use the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 1.5.1\n",
      "# Stream Variants\n",
      "\n",
      "The different variants of the stream or Thread that can be sent to the client.\n",
      "They are always sent as JSON strings in the format `{\"variant\": \"variant_name\", \"content\": \"content\"}`.\n",
      "\n",
      "User: The input of the user, as a String.\n",
      "\n",
      "Assistant: The output of the Assistant, as a String. Often Markdown, because the LLM can output Markdown.\n",
      "Multiple messages of this variant after each other belong to the same message, but are broken up due to the stream.\n",
      "\n",
      "Code: The code that the Assistant generated, as a String. It will be executed on the backend.\n",
      "Currently, only Python is supported. The content is not formatted.\n",
      "Due to how the LLM calls the code_interpreter, it will be contained within a json object in the following format:\n",
      "`{\"variant\": \"Code\", \"content\": \"{\\\"code\\\":\\\"LLM Code here\\\"}\"`\n",
      "\n",
      "CodeOutput: The output of the code that was executed, as a String. Also not formatted.\n",
      "\n",
      "Image: An image that was generated during the conversation, as a String. The image is Base64 encoded.\n",
      "An example of this would be a matplotlib plot. The image format should always be PNG.\n",
      "\n",
      "ServerError: An error that occured on the server(backend) side, as a String. Contains the error message.\n",
      "The client should realize that this error occured and handle it accordingly; ServerErrors should immeadiately be followed by a StreamEnd.\n",
      "\n",
      "OpenAI Error: An error that occured on the OpenAI side, as a String. Contains the error message.\n",
      "These are often for the rate limits, but can also be for other things, i.E. if the API is down or a tool call took too long.\n",
      "\n",
      "CodeError: The Code from the LLM could not be executed or there was some other error while setting up the code execution.\n",
      "\n",
      "StreamEnd: The Stream ended. Contains a reason as a String. This is always the last message of a stream.\n",
      "If the last message is not a StreamEnd but the stream ended, it's an error from the server side and needs to be fixed.\n",
      "\n",
      "ServerHint: The Server hints something to the client. This is primarily used for giving the thread_id, but also for warnings.\n",
      "The Content is in JSON format, with the key being the hint and the value being the content. Currently, only the keys \"thread_id\" and \"warning\" are used.\n",
      "An example for a ServerHint packet would be `{\"variant\": \"ServerHint\", \"content\": \"{\\\"thread_id\\\":\\\"1234\\\"}\"}`.\n",
      "That means that the content needs to be parsed as JSON to get the actual content.\n",
      "\n",
      "# Ping\n",
      "Simply returns a short description of the server's capabilities as well as the backend version.\n",
      "This is in the JSON format and contains three keys: the version, streamvariants and all the endpoint specs in a list.\n",
      "\n",
      "The version number is in the form of \"Version: x.y.z\" where x.y.z is the version of the backend. This follows the rules of SemVer.\n",
      "\n",
      "The Endpoints all have four keys: name, methods, params and return_type.\n",
      "\n",
      "This endpoint can be used to check whether the server is running and whether the interal model of the client matches the server's.\n",
      "\n",
      "# Docs\n",
      "Returns the documentation for the API.\n",
      "\n",
      "Takes no arguments and returns a string with the documentation.\n",
      "\n",
      "# Get Thread\n",
      "Returns the content of a thread as a Json of List of Strings.\n",
      "\n",
      "As arguments, it takes in a `thread_id` and an `auth_key`.\n",
      "\n",
      "The thread id is the unique identifier for the thread, given to the client when the stream started in a ServerHint variant.\n",
      "\n",
      "The auth key needs to match the one on the backend for the request to be authorized.\n",
      "To get the auth key, the user needs to contact the backend administrator.\n",
      "\n",
      "If the auth key is not given or does not match the one on the backend, an Unauthorized response is returned.\n",
      "\n",
      "If the thread id is not given, a BadRequest response is returned.\n",
      "\n",
      "If the thread with the given id is not found, a NotFound response is returned.\n",
      "\n",
      "If the thread is found but cannot be read or cannot be displayed, an InternalServerError response is returned.\n",
      "\n",
      "# Stream Response\n",
      "Takes in a thread_id, an input and an auth_key and returns a stream of StreamVariants and their content.\n",
      "\n",
      "The thread_id is the unique identifier for the thread, given to the client when the stream started in a ServerHint variant.\n",
      "If it's empty or not given, a new thread is created.\n",
      "\n",
      "The stream consists of StreamVariants and their content. See the different Stream Variants above.\n",
      "If the stream creates a new thread, the new thread_id will be sent as a ServerHint.\n",
      "The stream always ends with a StreamEnd event, unless a server error occurs.\n",
      "\n",
      "A usual stream consists mostly of Assistant messages many times a second. This is to give the impression of a real-time conversation.\n",
      "\n",
      "If the input is not given, a BadRequest response is returned.\n",
      "\n",
      "If the auth_key is not given or does not match the one on the backend, an Unauthorized response is returned.\n",
      "\n",
      "If the thread_id is blank but does not point to an existing thread, an InternalServerError response is returned.\n",
      "\n",
      "If the stream fails due to something else on the backend, an InternalServerError response is returned.\n",
      "\n",
      "# Stop\n",
      "Stops the conversation with the given thread ID as soon as possible.\n",
      "\n",
      "Takes in a `thread_id` and an `auth_key`.\n",
      "The thread_id identifies the conversation to stop.\n",
      "The auth_key needs to match the one on the backend for the request to be authorized.\n",
      "\n",
      "If the auth key is not given or does not match the one on the backend, an Unauthorized response is returned.\n",
      "\n",
      "If the thread id is not given, a BadRequest response is returned.\n",
      "\n",
      "If there is an error stopping the conversation, an InternalServerError response is returned.\n"
     ]
    }
   ],
   "source": [
    "url = base_url + '/docs'\n",
    "\n",
    "response = requests.get(url)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## /getthread\n",
    "`/getthread` is for retrieving a past thread by thread_id. It requires as query arguments both the thread_id as well as the auth_key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread not found.\n"
     ]
    }
   ],
   "source": [
    "# Try to get a thread\n",
    "# Read the AUTH_KEY from the environment variable AUTH_KEY\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "auth_key = os.getenv('AUTH_KEY')\n",
    "\n",
    "auth_string = \"&auth_key=\" + auth_key\n",
    "url = base_url + '/getthread?thread_id=1' + auth_string\n",
    "response = requests.get(url)\n",
    "\n",
    "print(response.text) # Thread ID was not found, because it wasn't created yet. The real thread_ids are strings, not integers. See below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## /streamresponse\n",
    "The `/streamresponse` is the main part of the API. It takes in the users input as a string, the auth_key and optionally the thread_id.\n",
    "\n",
    "If the thread_id is given, the conversation is continued where that thread last left off. If it isn't given, a new thread is started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"variant\":\"ServerHint\",\"content\":\"{\\\\\"thread_id\\\\\": \\\\\"IwbAgfDGwK7kx6rSN3jC6UeVzTsVnRdW\\\\\"}\"}'\n",
      "b'{\"variant\":\"Code\",\"content\":[\"\",\"call_9b8D4q39OK0K0B0bKRvpheDd\"]}'\n",
      "b'{\"variant\":\"Code\",\"content\":[\"{\\\\\"\",\"call_9b8D4q39OK0K0B0bKRvpheDd\"]}'\n",
      "b'{\"variant\":\"Code\",\"content\":[\"code\",\"call_9b8D4q39OK0K0B0bKRvpheDd\"]}'\n",
      "b'{\"variant\":\"Code\",\"content\":[\"\\\\\":\\\\\"\",\"call_9b8D4q39OK0K0B0bKRvpheDd\"]}'\n",
      "b'{\"variant\":\"Code\",\"content\":[\"result\",\"call_9b8D4q39OK0K0B0bKRvpheDd\"]}'\n",
      "b'{\"variant\":\"Code\",\"content\":[\" =\",\"call_9b8D4q39OK0K0B0bKRvpheDd\"]}'\n",
      "b'{\"variant\":\"Code\",\"content\":[\" \",\"call_9b8D4q39OK0K0B0bKRvpheDd\"]}'\n",
      "b'{\"variant\":\"Code\",\"content\":[\"128\",\"call_9b8D4q39OK0K0B0bKRvpheDd\"]}'\n",
      "b'{\"variant\":\"Code\",\"content\":[\"673\",\"call_9b8D4q39OK0K0B0bKRvpheDd\"]}'\n",
      "b'{\"variant\":\"Code\",\"content\":[\"2\",\"call_9b8D4q39OK0K0B0bKRvpheDd\"]}'\n",
      "b'{\"variant\":\"Code\",\"content\":[\" *\",\"call_9b8D4q39OK0K0B0bKRvpheDd\"]}'\n",
      "b'{\"variant\":\"Code\",\"content\":[\" \",\"call_9b8D4q39OK0K0B0bKRvpheDd\"]}'\n",
      "b'{\"variant\":\"Code\",\"content\":[\"298\",\"call_9b8D4q39OK0K0B0bKRvpheDd\"]}'\n",
      "b'{\"variant\":\"Code\",\"content\":[\"432\",\"call_9b8D4q39OK0K0B0bKRvpheDd\"]}'\n",
      "b'{\"variant\":\"Code\",\"content\":[\"44\",\"call_9b8D4q39OK0K0B0bKRvpheDd\"]}'\n",
      "b'{\"variant\":\"Code\",\"content\":[\"\\\\\\\\n\",\"call_9b8D4q39OK0K0B0bKRvpheDd\"]}'\n",
      "b'{\"variant\":\"Code\",\"content\":[\"result\",\"call_9b8D4q39OK0K0B0bKRvpheDd\"]}'\n",
      "b'{\"variant\":\"Code\",\"content\":[\"\\\\\"}\",\"call_9b8D4q39OK0K0B0bKRvpheDd\"]}'\n",
      "b'{\"variant\":\"CodeOutput\",\"content\":[\"38400257038608\",\"call_9b8D4q39OK0K0B0bKRvpheDd\"]}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\"\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\"The\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\" product\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\" of\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\" \"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\"1\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\",\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\"286\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\",\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\"732\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\" and\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\" \"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\"29\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\",\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\"843\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\",\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\"244\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\" is\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\" \"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\"38\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\",\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\"400\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\",\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\"257\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\",\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\"038\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\",\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\"608\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\".\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\" If\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\" you\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\" have\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\" any\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\" more\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\" calculations\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\" or\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\" questions\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\",\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\" feel\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\" free\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\" to\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\" ask\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\"!\"}'\n",
      "b'{\"variant\":\"StreamEnd\",\"content\":\"Generation complete\"}'\n"
     ]
    }
   ],
   "source": [
    "# user_input = \"This is a test. Please respond with \\\"200 OK\\\" and exit.\"\n",
    "user_input = \"This is a test. Use the code_interpreter function to calculate the product of 1286732 and 29843244 and return the result.\"\n",
    "url = base_url + '/streamresponse?input=' + user_input + auth_string # leaving out the thread_id spawns a new thread\n",
    "\n",
    "response = requests.get(url, stream=True) # The response can be streamed or gotten all at once.\n",
    "complete_response = [] # The stream gets consumed when streamed, we'll store it here. Note that python chunks the response, so it might be split up if it's long.\n",
    "for delta in response:\n",
    "    print(delta)\n",
    "    complete_response.append(delta.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"thread_id\": \"IwbAgfDGwK7kx6rSN3jC6UeVzTsVnRdW\"}\n",
      "IwbAgfDGwK7kx6rSN3jC6UeVzTsVnRdW\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "responses = []\n",
    "for delta in complete_response:\n",
    "    try:\n",
    "        response = json.loads(delta)\n",
    "        responses.append(response)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error decoding JSON: \" + delta)\n",
    "\n",
    "# Now we can simply access the content of the response.\n",
    "\n",
    "# We saw that in this case the first delta was a ServerHint containing the thread_id in JSON format.\n",
    "thread_id_content = responses[0][\"content\"] \n",
    "print(thread_id_content)\n",
    "# Deserialize the JSON content of the ServerHint\n",
    "thread_id = json.loads(thread_id_content)[\"thread_id\"]\n",
    "print(thread_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"variant\":\"ServerHint\",\"content\":\"{\\\"thread_id\\\": \\\"IwbAgfDGwK7kx6rSN3jC6UeVzTsVnRdW\\\"}\"},{\"variant\":\"User\",\"content\":\"This is a test. Use the code_interpreter function to calculate the product of 1286732 and 29843244 and return the result.\"},{\"variant\":\"Code\",\"content\":[\"{\\\"code\\\":\\\"result = 1286732 * 29843244\\\\nresult\\\"}\",\"call_9b8D4q39OK0K0B0bKRvpheDd\"]},{\"variant\":\"CodeOutput\",\"content\":[\"38400257038608\",\"call_9b8D4q39OK0K0B0bKRvpheDd\"]},{\"variant\":\"Assistant\",\"content\":\"The product of 1,286,732 and 29,843,244 is 38,400,257,038,608. If you have any more calculations or questions, feel free to ask!\"},{\"variant\":\"StreamEnd\",\"content\":\"Generation complete\"}]\n",
      "['The product of 1,286,732 and 29,843,244 is 38,400,257,038,608. If you have any more calculations or questions, feel free to ask!']\n"
     ]
    }
   ],
   "source": [
    "# Now that we have the thread_id, we can also test the /getthread endpoint\n",
    "url = base_url + '/getthread?thread_id=' + thread_id + auth_string\n",
    "response = requests.get(url)\n",
    "\n",
    "print(response.text) # This time we should get the thread content.\n",
    "response_json = json.loads(response.text)\n",
    "\n",
    "# Example: extract everything the Assistant said\n",
    "assistant_messages = [message[\"content\"] for message in response_json if message[\"variant\"] == \"Assistant\"]\n",
    "print(assistant_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## /stop\n",
    "`/stop` is a simple endpoint that stops the generation of a running thread as soon as the server recieves the request. \n",
    "\n",
    "It takes in the thread_id as well as the auth_key. It can use the `get` or `post` method, both work identically. \n",
    "\n",
    "Also note that because this is a test environment, the maximum length of the assistant's answer is comparitively short, so stopping a thread can not be demonstrated well here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation not found.\n"
     ]
    }
   ],
   "source": [
    "# Short example of the /stop endpoint\n",
    "url = base_url + '/stop?thread_id=' + thread_id + auth_string\n",
    "\n",
    "response = requests.get(url) # could also be post\n",
    "print(response.text) # The conversation was not found, because it already stopped. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
