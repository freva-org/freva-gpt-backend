{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking whether the API is up\n",
    "The `/ping` endpoint (`/help` is equivalent) is meant to give an overview over whether the API is up, what version it is running and what state it has. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 1.1.1\n",
      "Streamvariants=User,Assistant,Code,CodeOutput,Image,ServerError,OpenAIError,CodeError,StreamEnd,ServerHint\n",
      "ping:get,,String\n",
      "docs:get,,String\n",
      "getthread:get,thread_id=String&auth_key=String,Json{List{Variant:Streamvariant=String,Content:String}}\n",
      "streamresponse:get,thread_id=Optional{String}&input=String&auth_key=String,Stream{Json{Variant:Streamvariant=String,Content:String}}\n",
      "stop:post+get,thread_id=String&auth_key=String,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example use of the /ping endpoint\n",
    "import requests\n",
    "\n",
    "base_url = 'http://localhost:8502' # Change the URL to vader5 if you are running it against vader5.\n",
    "url = base_url + '/ping'\n",
    "response = requests.get(url)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the ping response\n",
    "The ping response is largely for manual checking, but can also be used to verify that the interface to the server is as expected. \n",
    "A ping response consists of one line explaining the version, which follows [semver](https://semver.org/), \n",
    "followed by one line listing all Stream variants that the server can respond with. See the Stream variants header below. \n",
    "\n",
    "All consequent lines describe the API endpoints and how to use them. These contain three values, seperated by commas. First, the requests that are accepted, i.E. \"get\" or \"post+get\" in the case of the `stop` endpoint.\n",
    "Second, the input parameters as query parameters. The format is \"key=type\", where type can for example be \"String\" or \"Optional\", marking that that parameter can be ommitted. \n",
    "Third is the return type of that endpoint. These are explained at the seperate endpoint explenations below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream variants\n",
    "To give the client as much information about the conversation with the LLM as necessary, seperate Stream variants are used to mark different events or participants of the conversation. \n",
    "\n",
    "As of version 1.1.1, there are 10 Stream variants: User,Assistant,Code,CodeOutput,Image,ServerError,OpenAIError,CodeError,StreamEnd,ServerHint; as can also be read when running `/docs`. \n",
    "Changing the behaviour of variants requires a major version bump and adding variants requires a minor version bump in accordance to semver. \n",
    "This mean that as long as the major version of the backend matches the major version the client was written for, the client can expect its handlin of the stream variants to be correct (or suffient, as it might ignore new variants).\n",
    "All unknown Stream variants can be safely ignored by the client. \n",
    "\n",
    "These are the current Stream variants in Version 1.1.*:\n",
    "- User: The input of the user, as a String\n",
    "- Assistant: The output of the Assistant, as a String. Often Markdown, because the LLM can output Markdown.\n",
    "Multiple messages of this variant after each other belong to the same message, but are broken up due to the stream.\n",
    "- Code: The code that the Assistant generated, as a String. It will be executed on the backend.\n",
    "Currently, only Python is supported. The content is not formatted.\n",
    "- CodeOutput: The output of the code that was executed, as a String. Also not formatted.\n",
    "- Image: An image that was generated during the conversation, as a String. The image is Base64 encoded.\n",
    "An example of this would be a matplotlib plot.\n",
    "- ServerError: An error that occured on the server(backend) side, as a String. Contains the error message.\n",
    "The client should realize that this error occured and handle it accordingly; most ServerErrors are immeadiately followed by a StreamEnd.\n",
    "- OpenAI Error: An error that occured on the OpenAI side, as a String. Contains the error message.\n",
    "These are often for the rate limits, but can also be for other things, i.E. if the API is down.\n",
    "- CodeError: The Code from the LLM could not be executed or there was some other error while setting up the code execution.\n",
    "- StreamEnd: The Stream ended. Contains a reason as a String. This is always the last message of a stream.\n",
    "If the last message is not a StreamEnd but the stream ended, it's an error from the server side and needs to be fixed.\n",
    "- ServerHint: The Server hints something to the client. This is primarily used for giving the thread_id.\n",
    "The Content is in the format `<key>:<value>`, for now the key is \"thread_id\" and the value is the thread_id.\n",
    "Might be used for other things in the future. If the client receives a ServerHint with an unknown key, it should log a warning, but not crash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "To make sure that not everyone on the network can simply use the API, a base authentication is in place that requires the client to send, with every request, an auth_key that has to match the one on the server. \n",
    "\n",
    "Currently (version 1.1.1), for testing purposes, the auth_key on the server is set to \"qA94VhroHMHFN55inWgfAAkt1WEmzQ4J\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## /ping\n",
    "`/ping` and `/help` are meant to allow the client to check whether the server is up and to get the version as well as a quick overview over the API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## /docs\n",
    "`/docs` is supposed to only be used manually to check how to use the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 1.1.1\n",
      "\n",
      "# Stream Variants\n",
      "\n",
      "The different variants of the stream or Thread that can be sent to the client.\n",
      "They are always sent as JSON strings in the format `{\"variant\": \"variant_name\", \"content\": \"content\"}`.\n",
      "\n",
      "User: The input of the user, as a String.\n",
      "\n",
      "Assistant: The output of the Assistant, as a String. Often Markdown, because the LLM can output Markdown.\n",
      "Multiple messages of this variant after each other belong to the same message, but are broken up due to the stream.\n",
      "\n",
      "Code: The code that the Assistant generated, as a String. It will be executed on the backend.\n",
      "Currently, only Python is supported. The content is not formatted.\n",
      "\n",
      "CodeOutput: The output of the code that was executed, as a String. Also not formatted.\n",
      "\n",
      "Image: An image that was generated during the conversation, as a String. The image is Base64 encoded.\n",
      "An example of this would be a matplotlib plot.\n",
      "\n",
      "ServerError: An error that occured on the server(backend) side, as a String. Contains the error message.\n",
      "The client should realize that this error occured and handle it accordingly; most ServerErrors are immeadiately followed by a StreamEnd.\n",
      "\n",
      "OpenAI Error: An error that occured on the OpenAI side, as a String. Contains the error message.\n",
      "These are often for the rate limits, but can also be for other things, i.E. if the API is down.\n",
      "\n",
      "CodeError: The Code from the LLM could not be executed or there was some other error while setting up the code execution.\n",
      "\n",
      "StreamEnd: The Stream ended. Contains a reason as a String. This is always the last message of a stream.\n",
      "If the last message is not a StreamEnd but the stream ended, it's an error from the server side and needs to be fixed.\n",
      "\n",
      "ServerHint: The Server hints something to the client. This is primarily used for giving the thread_id.\n",
      "The Content is in the format `<key>:<value>`, for now the key is \"thread_id\" and the value is the thread_id.\n",
      "Might be used for other things in the future. If the client receives a ServerHint with an unknown key, it should log a warning, but not crash.\n",
      "# PING:\n",
      "\n",
      "Simply returns a short description of the server's capabilities as well as the backend version.\n",
      "The first line is always \"Version: x.y.z\" where x.y.z is the version of the backend. This follows the rules of SemVer.\n",
      "\n",
      "The second line describes all Streamvariants that the server can return.\n",
      "\n",
      "From the third line onwards, every line describes an API endpoint. The format is:\n",
      "NAME:METHOD(S),ARGUMENTS,RETURN_TYPE\n",
      "# DOCS:\n",
      "\n",
      "Returns the documentation for the API.\n",
      "\n",
      "Takes no arguments and returns a string with the documentation.\n",
      "# GETTHREADS:\n",
      "\n",
      "Returns the content of a thread as a Json of List of Strings. \n",
      "\n",
      "As arguments, it takes in a `thread_id` and an `auth_key`.\n",
      "\n",
      "The thread id is the unique identifier for the thread, given to the client when the stream started in a ServerHint variant.\n",
      "\n",
      "The auth key needs to match the one on the backend for the request to be authorized.\n",
      "To get the auth key, the user needs to contact the backend administrator.\n",
      "\n",
      "If the auth key is not given or does not match the one on the backend, an Unauthorized response is returned.\n",
      "\n",
      "If the thread id is not given, a BadRequest response is returned.\n",
      "\n",
      "If the thread with the given id is not found, a NotFound response is returned.\n",
      "\n",
      "If the thread is found but cannot be read or cannot be displayed, an InternalServerError response is returned.\n",
      "# STREAMRESPONSE:\n",
      "\n",
      "Takes in a thread_id, an input and an auth_key and returns a stream of StreamVariants and their content.\n",
      "\n",
      "The thread_id is the unique identifier for the thread, given to the client when the stream started in a ServerHint variant.\n",
      "If it's empty or not given, a new thread is created.\n",
      "\n",
      "The stream consists of StreamVariants and their content. See the different Stream Variants above. \n",
      "If the stream creates a new thread, the new thread_id will be sent as a ServerHint.\n",
      "The stream always ends with a StreamEnd event, unless a server error occurs.\n",
      "\n",
      "A usual stream cosists mostly of Assistant messages many times a second. This is to give the impression of a real-time conversation.\n",
      "\n",
      "If the input is not given, a BadRequest response is returned.\n",
      "\n",
      "If the auth_key is not given or does not match the one on the backend, an Unauthorized response is returned.\n",
      "\n",
      "If the thread_id does not point to an existing thread, an InternalServerError response is returned.\n",
      "\n",
      "If the stream fails due to something else on the backend, an InternalServerError response is returned.\n",
      "# STOP:\n",
      "\n",
      "Stops the conversation with the given thread ID as soon as possible.\n",
      "\n",
      "Takes in a `thread_id` and an `auth_key`.\n",
      "The thread_id identifies the conversation to stop.\n",
      "The auth_key needs to match the one on the backend for the request to be authorized.\n",
      "\n",
      "If the auth key is not given or does not match the one on the backend, an Unauthorized response is returned.\n",
      "\n",
      "If the thread id is not given, a BadRequest response is returned.\n",
      "\n",
      "If there is an error stopping the conversation, an InternalServerError response is returned.\n"
     ]
    }
   ],
   "source": [
    "url = base_url + '/docs'\n",
    "\n",
    "response = requests.get(url)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## /getthread\n",
    "`/getthread` is for retrieving a past thread by thread_id. It requires as query arguments both the thread_id as well as the auth_key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread not found.\n"
     ]
    }
   ],
   "source": [
    "# Try to get a thread\n",
    "auth_key = \"qA94VhroHMHFN55inWgfAAkt1WEmzQ4J\"\n",
    "auth_string = \"&auth_key=\" + auth_key\n",
    "url = base_url + '/getthread?thread_id=1' + auth_string\n",
    "response = requests.get(url)\n",
    "\n",
    "print(response.text) # Thread ID was not found, because it wasn't created yet. The real thread_ids are strings, not integers. See below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## /streamresponse\n",
    "The `/streamresponse` is the main part of the API. It takes in the users input as a string, the auth_key and optionally the thread_id.\n",
    "\n",
    "If the thread_id is given, the conversation is continued where that thread last left off. If it isn't given, a new thread is started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"variant\":\"ServerHint\",\"content\":\"thread_id:3MNpwfe9rr9zFyDDJgAbgD8jAtaO8C23\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\"\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\"200\"}'\n",
      "b'{\"variant\":\"Assistant\",\"content\":\" OK\"}'\n",
      "b'{\"variant\":\"StreamEnd\",\"content\":\"Generation complete\"}'\n"
     ]
    }
   ],
   "source": [
    "user_input = \"This is a test. Please respond with \\\"200 OK\\\" and exit.\"\n",
    "url = base_url + '/streamresponse?input=' + user_input + auth_string # leaving out the thread_id spawns a new thread\n",
    "\n",
    "response = requests.get(url, stream=True) # The response can be streamed or gotten all at once.\n",
    "complete_response = [] # The stream gets consumed when streamed, we'll store it here.\n",
    "for delta in response:\n",
    "    print(delta)\n",
    "    complete_response.append(delta.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread ID: 3MNpwfe9rr9zFyDDJgAbgD8jAtaO8C23\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "responses = []\n",
    "for delta in complete_response:\n",
    "    try:\n",
    "        response = json.loads(delta)\n",
    "        responses.append(response)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error decoding JSON: \" + delta)\n",
    "\n",
    "# Now we can simply access the content of the response.\n",
    "\n",
    "# We saw that in this case the first delta was a ServerHint containing the thread_id in the format \"thread_id:THREAD_ID\"\n",
    "thread_id_content = responses[0][\"content\"] \n",
    "thread_id = thread_id_content.split(\":\")[1].strip()\n",
    "print(\"Thread ID: \" + thread_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"variant\":\"ServerHint\",\"content\":\"thread_id:3MNpwfe9rr9zFyDDJgAbgD8jAtaO8C23\"},{\"variant\":\"User\",\"content\":\"This is a test. Please respond with \\\\\\\"200 OK\\\\\\\" and exit.\"},{\"variant\":\"Assistant\",\"content\":\"200 OK\"},{\"variant\":\"StreamEnd\",\"content\":\"Generation complete\"}]\n",
      "['200 OK']\n"
     ]
    }
   ],
   "source": [
    "# Now that we have the thread_id, we can also test the /getthread endpoint\n",
    "url = base_url + '/getthread?thread_id=' + thread_id + auth_string\n",
    "response = requests.get(url)\n",
    "\n",
    "print(response.text) # This time we should get the thread content.\n",
    "response_json = json.loads(response.text)\n",
    "\n",
    "# Example: extract everything the Assistant said\n",
    "assistant_messages = [message[\"content\"] for message in response_json if message[\"variant\"] == \"Assistant\"]\n",
    "print(assistant_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## /stop\n",
    "`/stop` is a simple endpoint that stops the generation of a running thread as soon as the server recieves the request. \n",
    "\n",
    "It takes in the thread_id as well as the auth_key. It can use the `get` or `post` method, both work identically. \n",
    "\n",
    "Also note that because this is a test environment, the maximum length of the assistant's answer is comparitively short, so stopping a thread can not be demonstrated well here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation not found.\n"
     ]
    }
   ],
   "source": [
    "# Short example of the /stop endpoint\n",
    "url = base_url + '/stop?thread_id=' + thread_id + auth_string\n",
    "\n",
    "response = requests.get(url) # could also be post\n",
    "print(response.text) # The conversation was not found, because it already stopped. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
