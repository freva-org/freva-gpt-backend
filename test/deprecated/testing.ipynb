{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing \n",
    "(Mainly locally, I don't know a method to get python notebook on vader5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "base_url = 'http://localhost:8502/api/chatbot'\n",
    "\n",
    "load_dotenv()\n",
    "auth_key = os.getenv(\"AUTH_KEY\")\n",
    "auth_string = \"&auth_key=\" + auth_key + \"&user_id=testing\" # Only for testing\n",
    "# In Version 1.6.1, the freva_config also needs to be set to a specific path. We won't be using this for now.\n",
    "auth_string = auth_string + \"&freva_config=\" + \"Cargo.toml\" # Dummy value\n",
    "auth_string = auth_string + \"&user_id=\" + \"testing\" # Dummy value\n",
    "\n",
    "# As a test, ping the server\n",
    "response = requests.get(base_url + \"/ping\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also read the docs\n",
    "response = requests.get(base_url + \"/docs\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which chatbots are available\n",
    "response = requests.get(base_url + \"/availablechatbots?\" + auth_string)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Checking whether the code interpreter can be called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# user_input = \"This is a test request for your basic functionality. Please respond with (200 Ok) and exit.\"\n",
    "# user_input = \"This is a test request for your capabilities of using the code_interpreter tool. Please use the code_interpreter tool to run the following code exactly and only once: \\\"print('Hello, World!', flush=True)\\\".\"\n",
    "# user_input = \"This is a test regarding your capabilities of using the code_interpreter tool and whether it supports matplotlib. Please use the code_interpreter tool to use matplotlib to visualize a sine wave from -2pi to +2pi.\"\n",
    "# user_input = \"This is a test regarding your capabilities of using the code_interpreter tool and whether it supports matplotlib. Please use the code_interpreter tool to run the following code: \\\"import numpy as np\\nimport matplotlib.pyplot as plt\\nimport matplotlib\\nmatplotlib.use('agg')\\nt = np.linspace(-2 * np.pi, 2 * np.pi, 100)\\nsine_wave = np.sin(t)\\nplt.figure(figsize=(10, 5))\\nplt.plot(t, sine_wave, label='Sine Wave')\\nplt.title('Sine Wave from -2π to 2π')\\nplt.xlabel('Angle (radians)')\\nplt.ylabel('Sine value')\\nplt.axhline(0, color='black', linewidth=0.5, linestyle='--')\\nplt.axvline(0, color='black', linewidth=0.5, linestyle='--')\\nplt.grid()\\nplt.legend()\\nplt.show()\\\".\"\n",
    "# The below input doesn't use the corerct matplotlib backend, so it might do unexpected things.\n",
    "user_input = \"This is a test regarding your capabilities of using the code_interpreter tool and whether it supports matplotlib. Please use the code_interpreter tool to run the following code: \\\"import numpy as np\\nimport matplotlib.pyplot as plt\\nt = np.linspace(-2 * np.pi, 2 * np.pi, 100)\\nsine_wave = np.sin(t)\\nplt.figure(figsize=(10, 5))\\nplt.plot(t, sine_wave, label='Sine Wave')\\nplt.title('Sine Wave from -2π to 2π')\\nplt.xlabel('Angle (radians)')\\nplt.ylabel('Sine value')\\nplt.axhline(0, color='black', linewidth=0.5, linestyle='--')\\nplt.axvline(0, color='black', linewidth=0.5, linestyle='--')\\nplt.grid()\\nplt.legend()\\nplt.show()\\\".\"\n",
    "url = base_url + '/streamresponse?input=' + user_input + \"&chatbot=gpt-4o-mini\" + auth_string # leaving out the thread_id spawns a new thread\n",
    "\n",
    "response = requests.get(url, stream=True) # The response can be streamed or gotten all at once.\n",
    "complete_response = [] # The stream gets consumed when streamed, we'll store it here.\n",
    "\n",
    "# Note that this way to cunsume the response chunks it and those chunks are surprisingly small, cutting up the response.\n",
    "# A better way would be `for delta in response.iter_content(chunk_size=1024):`\n",
    "for delta in response:\n",
    "    print(delta)\n",
    "    complete_response.append(delta.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some elements are cut off early if the chunk size is not specified or too small, so to reconstruct, we'll need to append all element that don't start with \"{\" to the previous element.\n",
    "reconstructed_response = []\n",
    "for i in range(len(complete_response)):\n",
    "    if complete_response[i][0] == \"{\":\n",
    "        reconstructed_response.append(complete_response[i])\n",
    "    else:\n",
    "        reconstructed_response[-1] += complete_response[i]\n",
    "\n",
    "for delta in reconstructed_response:\n",
    "    print(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can parse the response as JSON\n",
    "import json\n",
    "response_json = [json.loads(x) for x in reconstructed_response]\n",
    "for element in response_json:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use this to easily extract only the Assistant's responses\n",
    "assistant_responses = [x for x in response_json if x[\"variant\"] == \"Assistant\"]\n",
    "print(\"\".join([x[\"content\"] for x in assistant_responses])) # This will print the Assistant's responses in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or the generated Code\n",
    "code_responses = [x for x in response_json if x[\"variant\"] == \"Code\"]\n",
    "print(\"\".join([x[\"content\"][0] for x in code_responses]).replace(\"\\\\\\n\",\"\\n\")) # This will print the generated code in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or all coded output\n",
    "code_output_responses = [x[\"content\"][0] for x in response_json if x[\"variant\"] == \"CodeOutput\"]\n",
    "for error in code_output_responses:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also extrace the image, if there is one\n",
    "image_responses = [x[\"content\"] for x in response_json if x[\"variant\"] == \"Image\"]\n",
    "# it's base64, so we can use IPython to display it\n",
    "from IPython.display import display, Image\n",
    "from base64 import b64decode\n",
    "for image in image_responses:\n",
    "    display(Image(data=b64decode(image), format='png')) # only for testing. This will display the image in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the code_interpreter tool: This should return \"Hello\\nWorld\\n!\"\n",
    "user_input = \"Please use the code_interpreter tool to run the following code exactly and only once: \\\"print('Hello\\\\nWorld\\\\n!', flush=True)\\\".\"\n",
    "url = base_url + '/streamresponse?input=' + user_input + \"&chatbot=gpt-4o-mini\" + auth_string # leaving out the thread_id spawns a new thread\n",
    "hello_world_response = requests.get(url, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve thread_id\n",
    "hello_world_response_reconstructed = []\n",
    "for delta in hello_world_response:\n",
    "    if delta.decode(\"utf-8\")[0] == \"{\":\n",
    "        hello_world_response_reconstructed.append(delta.decode(\"utf-8\"))\n",
    "    else:\n",
    "        hello_world_response_reconstructed[-1] += delta.decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "hello_world_response_json = [json.loads(x) for x in hello_world_response_reconstructed]\n",
    "hw_thread_id = json.loads(hello_world_response_json[0][\"content\"])[\"thread_id\"]\n",
    "print(hw_thread_id)\n",
    "\n",
    "# Also retrieve all variants seperately\n",
    "hello_world_stream_output = [x[\"content\"][0] for x in hello_world_response_json if x[\"variant\"] == \"CodeOutput\"]\n",
    "hello_world_stream_code = [x[\"content\"][0] for x in hello_world_response_json if x[\"variant\"] == \"Code\"]\n",
    "hello_world_stream_assistant = [x[\"content\"] for x in hello_world_response_json if x[\"variant\"] == \"Assistant\"]\n",
    "hello_world_stream_user = [x[\"content\"] for x in hello_world_response_json if x[\"variant\"] == \"User\"]\n",
    "\n",
    "print(\"\".join(hello_world_stream_output))\n",
    "print(\"\".join(hello_world_stream_code))\n",
    "print(\"\".join(hello_world_stream_assistant))\n",
    "print(\"\".join(hello_world_stream_user))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now also retrieve the response from the getThread endpoint\n",
    "url = base_url + '/getthread?thread_id=' + hw_thread_id + auth_string\n",
    "hello_world_thread_response = requests.get(url)\n",
    "hello_world_thread_json = hello_world_thread_response.json()\n",
    "hello_world_thread_text = hello_world_thread_response.text\n",
    "hello_world_thread_code_output = [x[\"content\"][0] for x in hello_world_thread_json if x[\"variant\"] == \"CodeOutput\"]\n",
    "hello_world_thread_code = [x[\"content\"][0] for x in hello_world_thread_json if x[\"variant\"] == \"Code\"]\n",
    "hello_world_thread_assistant = [x[\"content\"] for x in hello_world_thread_json if x[\"variant\"] == \"Assistant\"]\n",
    "hello_world_thread_user = [x[\"content\"] for x in hello_world_thread_json if x[\"variant\"] == \"User\"]\n",
    "\n",
    "\n",
    "print(\"\".join(hello_world_thread_code_output))\n",
    "print(\"\".join(hello_world_thread_code))\n",
    "print(\"\".join(hello_world_thread_assistant))\n",
    "print(\"\".join(hello_world_thread_user))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hello_world_thread_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the get_thread endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new thread asking it to add 2+2 in the code_interpreter\n",
    "user_input = \"Please add 2+2 in the code_interpreter tool.\"\n",
    "url = base_url + '/streamresponse?input=' + user_input + \"&chatbot=gpt-4o-mini\" + auth_string # leaving out the thread_id spawns a new thread\n",
    "response = requests.get(url, stream=True) # The response can be streamed or gotten all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compare the output between streamresponse and getthread, output the json from getthread\n",
    "recon_reponse = []\n",
    "temp = response.text.split(\"}{\")\n",
    "for i in range(len(temp)):\n",
    "    if i == 0:\n",
    "        recon_reponse.append(temp[i] + \"}\")\n",
    "    elif i == len(temp) - 1:\n",
    "        recon_reponse.append(\"{\" + temp[i])\n",
    "    else:\n",
    "        recon_reponse.append(\"{\" + temp[i] + \"}\")\n",
    "response_json = [json.loads(x) for x in recon_reponse]\n",
    "response_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the thread_id\n",
    "thread_id = json.loads(response_json[0][\"content\"])[\"thread_id\"]\n",
    "print(thread_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the thread_id to stream another response in the same thread\n",
    "user_input = \"Now please multiply that by 3.\"\n",
    "url = base_url + '/streamresponse?input=' + user_input + '&thread_id=' + thread_id + \"&chatbot=gpt-4o-mini\" + auth_string\n",
    "m3response = requests.get(url, stream=True)\n",
    "m3complete_response = [] # The stream gets consumed when streamed, we'll store it here.\n",
    "\n",
    "for delta in m3response:\n",
    "    m3complete_response.append(delta.decode(\"utf-8\"))\n",
    "\n",
    "m3reconstructed_response = []\n",
    "for i in range(len(m3complete_response)):\n",
    "    if m3complete_response[i][0] == \"{\":\n",
    "        m3reconstructed_response.append(m3complete_response[i])\n",
    "    else:\n",
    "        m3reconstructed_response[-1] += m3complete_response[i]\n",
    "\n",
    "m3response_json = [json.loads(x) for x in m3reconstructed_response]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3code = [x for x in m3response_json if x[\"variant\"] == \"Code\"]\n",
    "print(\"\".join([x[\"content\"][0] for x in m3code]).replace(\"\\\\\\n\",\"\\n\")) # This will print the generated code in order.\n",
    "m3assistant_responses = [x for x in m3response_json if x[\"variant\"] == \"Assistant\"]\n",
    "print(\"\".join([x[\"content\"] for x in m3assistant_responses])) # This will print the Assistant's responses in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the get_thread endpoint\n",
    "url = base_url + '/getthread?thread_id=' + thread_id + auth_string\n",
    "getthreadresponse = requests.get(url)\n",
    "getthreadresponse.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing the text of both responses. Both should be almost identical, but the getthread response should have collapsed many of the variants and is also a list, not a stream. \n",
    "print(response.text)\n",
    "print(getthreadresponse.text)\n",
    "getthreadresponse.json() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the Version 1.6.3, the code_interpreter now has persistent storage, so we can test that here.\n",
    "# We simply assign a variable in the code_interpreter and then retrieve it in a new thread.\n",
    "user_input = \"Please assign the value 42 to the variable x in the code_interpreter tool. After that, call the tool with the code \\\"print(x, flush=True)\\\", without assigning x again. It's a test for the presistance of data.\"\n",
    "url = base_url + '/streamresponse?input=' + user_input + \"&chatbot=gpt-4o-mini\" + auth_string # leaving out the thread_id spawns a new thread\n",
    "pers_response = requests.get(url, stream=True) # The response can be streamed or gotten all at once.\n",
    "pers_reconstructed_response = []\n",
    "for delta in pers_response:\n",
    "    if delta.decode(\"utf-8\")[0] == \"{\":\n",
    "        pers_reconstructed_response.append(delta.decode(\"utf-8\"))\n",
    "    else:\n",
    "        pers_reconstructed_response[-1] += delta.decode(\"utf-8\")\n",
    "\n",
    "pers_response_json = [json.loads(x) for x in pers_reconstructed_response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\".join([x[\"content\"][0] for x in pers_response_json if x[\"variant\"] == \"Code\"])) # Print the Code.\n",
    "print(\"\".join([x[\"content\"][0] for x in pers_response_json if x[\"variant\"] == \"CodeOutput\"])) # Print the CodeOutput.\n",
    "print(\"\".join([x[\"content\"] for x in pers_response_json if x[\"variant\"] == \"Assistant\"])) # This will print the Assistant's responses in order.\n",
    "pers_thread_id = json.loads(pers_response_json[0][\"content\"])[\"thread_id\"]\n",
    "print(pers_thread_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the Version 1.6.5, the capability to store xarray datasets between calls to the code_interpreter was added.\n",
    "# This is a test to see if it works.\n",
    "# We simply ask the LLM to generate a simple xarray dataset and then access it in a new code_interpreter call.\n",
    "user_input = \"Please generate a simple xarray dataset in the code_interpreter tool and print out the content. After that, call the tool with the code \\\"print(ds, flush=True)\\\", without generating the dataset again. It's a test for the presistance of data, specifically whether xarray Datasets also work.\"\n",
    "url = base_url + '/streamresponse?input=' + user_input + \"&chatbot=gpt-4o-mini\" + auth_string # leaving out the thread_id spawns a new thread\n",
    "xarray_response = requests.get(url, stream=True) # The response can be streamed or gotten all at once.\n",
    "xarray_reconstructed_response = []\n",
    "for delta in xarray_response:\n",
    "    if delta.decode(\"utf-8\")[0] == \"{\":\n",
    "        xarray_reconstructed_response.append(delta.decode(\"utf-8\"))\n",
    "    else:\n",
    "        xarray_reconstructed_response[-1] += delta.decode(\"utf-8\")\n",
    "\n",
    "xarray_response_json = [json.loads(x) for x in xarray_reconstructed_response]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "print(\"\".join([x[\"content\"][0] for x in xarray_response_json if x[\"variant\"] == \"Code\"])) # Print the Code.\n",
    "print(\"\".join([x[\"content\"][0] for x in xarray_response_json if x[\"variant\"] == \"CodeOutput\"])) # Print the CodeOutput.\n",
    "print(\"\".join([x[\"content\"] for x in xarray_response_json if x[\"variant\"] == \"Assistant\"])) # This will print the Assistant's responses in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Version 1.7.1, the streamresponse endpoint was updated to allow for the setting of which chatbot to use. This tests whether it can be used.\n",
    "# Stream a very simple request to llama instead of the default chatbot.\n",
    "user_input = \"This is a test request for your basic functionality. Please respond with (200 Ok) and exit.\"\n",
    "url = base_url + '/streamresponse?input=' + user_input + \"&chatbot=qwen2.5:3b\" + auth_string # leaving out the thread_id spawns a new thread\n",
    "llama_response = requests.get(url, stream=True) # The response can be streamed or gotten all at once.\n",
    "llama_complete_response = [] # The stream gets consumed when streamed, we'll store it here.\n",
    "\n",
    "for delta in llama_response:\n",
    "    llama_complete_response.append(delta.decode(\"utf-8\"))\n",
    "\n",
    "llama_reconstructed_response = []\n",
    "for i in range(len(llama_complete_response)):\n",
    "    if llama_complete_response[i][0] == \"{\":\n",
    "        llama_reconstructed_response.append(llama_complete_response[i])\n",
    "    else:\n",
    "        llama_reconstructed_response[-1] += llama_complete_response[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the response\n",
    "llama_response_json = [json.loads(x) for x in llama_reconstructed_response]\n",
    "print(\"\".join([x[\"content\"] for x in llama_response_json if x[\"variant\"] == \"Assistant\"])) # This will print the Assistant's responses in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream a request to qwen2.5:3b to calculate 2938429834 * 234987234 using the code_interpreter tool.\n",
    "user_input = \"Please use the code_interpreter tool to run `print(2938429834 * 234987234)`. Make sure to adhere to the JSON format!\"\n",
    "url = base_url + '/streamresponse?input=' + user_input + \"&chatbot=qwen2.5:3b\" + auth_string # leaving out the thread_id spawns a new thread\n",
    "qwen_response = requests.get(url, stream=True) # The response can be streamed or gotten all at once.\n",
    "qwen_complete_response = [] # The stream gets consumed when streamed, we'll store it here.\n",
    "\n",
    "for delta in qwen_response:\n",
    "    qwen_complete_response.append(delta.decode(\"utf-8\"))\n",
    "    print(delta.decode(\"utf-8\"))\n",
    "\n",
    "qwen_reconstructed_response = []\n",
    "for i in range(len(qwen_complete_response)):\n",
    "    if qwen_complete_response[i][0] == \"{\":\n",
    "        qwen_reconstructed_response.append(qwen_complete_response[i])\n",
    "    else:\n",
    "        qwen_reconstructed_response[-1] += qwen_complete_response[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print out the response\n",
    "qwen_response_json = [json.loads(x) for x in qwen_reconstructed_response]\n",
    "print(\"\".join([x[\"content\"] for x in qwen_response_json if x[\"variant\"] == \"Assistant\"])) # This will print the Assistant's responses in order.\n",
    "# for element in qwen_response_json:\n",
    "#     print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the heartbeat functionality introduced in 1.8.1 by sleeping for seven seconds in the code_interpreter tool.\n",
    "user_input = \"Please use the code_interpreter tool to run the following code: \\\"import time\\ntime.sleep(7)\\\".\"\n",
    "url = base_url + '/streamresponse?input=' + user_input + \"&chatbot=gpt-4o-mini\" + auth_string # leaving out the thread_id spawns a new thread\n",
    "\n",
    "response = requests.get(url, stream=True) # The response can be streamed or gotten all at once.\n",
    "heartbeat_response = [] # The stream gets consumed when streamed, we'll store it here.\n",
    "\n",
    "for delta in response:\n",
    "    print(delta)\n",
    "    heartbeat_response.append(delta.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the response\n",
    "reconstructed_response = []\n",
    "for i in range(len(heartbeat_response)):\n",
    "    if heartbeat_response[i][0] == \"{\":\n",
    "        reconstructed_response.append(heartbeat_response[i])\n",
    "    else:\n",
    "        reconstructed_response[-1] += heartbeat_response[i]\n",
    "\n",
    "# Extract the ServerHint variant\n",
    "server_hint = [x for x in reconstructed_response if json.loads(x)[\"variant\"] == \"ServerHint\"]\n",
    "print(server_hint)\n",
    "\n",
    "# The second element of the list contains the heartbeat message\n",
    "print(json.loads(server_hint[1])[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting in 1.8.3, stopping a conversation also stops all tool calls. This is a test to see if it works.\n",
    "# Because we can't be that sure that the tool call is running, we'll just have it wait 20 seconds, wait ourselves for 10 seconds and then stop the conversation.\n",
    "user_input = \"Please use the code_interpreter tool to run the following code: \\\"import time\\ntime.sleep(20)\\\". I will stop the conversation after 10 seconds. This is a test for whether stopping the conversation also stops the tool call.\"\n",
    "url = base_url + '/streamresponse?input=' + user_input + \"&chatbot=gpt-4o-mini\" + auth_string # leaving out the thread_id spawns a new thread\n",
    "\n",
    "response = requests.get(url, stream=True) # The response can be streamed or gotten all at once.\n",
    "# We can't just wait for the tool call to finish, but we need to read the first response to get the thread_id\n",
    "complete_response = [] # The stream gets consumed when streamed, we'll store it here.\n",
    "\n",
    "reconstructed_response = []\n",
    "\n",
    "first_run = True\n",
    "for delta in response:\n",
    "    decoded = delta.decode(\"utf-8\")\n",
    "    complete_response.append(decoded)\n",
    "    print(decoded)\n",
    "\n",
    "    # Only extract the thread_id from the first response\n",
    "    if not first_run :\n",
    "        continue\n",
    "\n",
    "    first_run = False\n",
    "    # We'll just expect that it didn't split the first response\n",
    "    first_content = json.loads(decoded)[\"content\"]\n",
    "    \n",
    "\n",
    "    # Extract the thread_id\n",
    "    stopping_thread_id = json.loads(first_content)[\"thread_id\"]\n",
    "    print(stopping_thread_id)\n",
    "\n",
    "    # Wait for 10 seconds\n",
    "    import time\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Stop the conversation\n",
    "    url = base_url + '/stop?thread_id=' + stopping_thread_id + auth_string\n",
    "    stop_response = requests.get(url)\n",
    "    print(stop_response.text)\n",
    "\n",
    "for i in range(len(complete_response)):\n",
    "    if complete_response[i][0] == \"{\":\n",
    "        reconstructed_response.append(complete_response[i])\n",
    "    else:\n",
    "        reconstructed_response[-1] += complete_response[i]\n",
    "\n",
    "reconstructed_response\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Version 1.8.9, the conversation is not corrupted anymore if the tool call is stopped. This is a test to see if it works.\n",
    "# (Continue the conversation with the ID from the previous test)\n",
    "user_input = \"Test: did the conversation continue after stopping the tool call?\"\n",
    "url = base_url + '/streamresponse?input=' + user_input + '&thread_id=' + stopping_thread_id + auth_string\n",
    "response = requests.get(url, stream=True) # The response can be streamed or gotten all at once.\n",
    "complete_response = [] # The stream gets consumed when streamed, we'll store it here.\n",
    "\n",
    "for delta in response:\n",
    "    # print(delta)\n",
    "    complete_response.append(delta.decode(\"utf-8\"))\n",
    "\n",
    "# Some elements are cut off early if the chunk size is not specified or too small, so to reconstruct, we'll need to append all element that don't start with \"{\" to the previous element.\n",
    "reconstructed_response = []\n",
    "for i in range(len(complete_response)):\n",
    "    if complete_response[i][0] == \"{\":\n",
    "        reconstructed_response.append(complete_response[i])\n",
    "    else:\n",
    "        reconstructed_response[-1] += complete_response[i]\n",
    "\n",
    "for delta in reconstructed_response:\n",
    "    print(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the improved syntax error reporting in Version 1.8.4\n",
    "user_input = \"Please use the code_interpreter tool to run the following code: \\\"print('Hello World'\\\". This is a test for the improved syntax error reporting. If a hint containing the syntax error is returned, the test is successful.\"\n",
    "url = base_url + '/streamresponse?input=' + user_input + \"&chatbot=gpt-4o-mini\" + auth_string # leaving out the thread_id spawns a new thread\n",
    "\n",
    "response = requests.get(url, stream=True) # The response can be streamed or gotten all at once.\n",
    "syntax_error_response = [] # The stream gets consumed when streamed, we'll store it here.\n",
    "\n",
    "for delta in response:\n",
    "    print(delta)\n",
    "    syntax_error_response.append(delta.decode(\"utf-8\"))\n",
    "\n",
    "# Reconstruct the response\n",
    "reconstructed_response = []\n",
    "for i in range(len(syntax_error_response)):\n",
    "    if syntax_error_response[i][0] == \"{\":\n",
    "        reconstructed_response.append(syntax_error_response[i])\n",
    "    else:\n",
    "        reconstructed_response[-1] += syntax_error_response[i]\n",
    "\n",
    "# Print the reconstructed response\n",
    "for delta in reconstructed_response:\n",
    "    print(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the CodeOutput and Assistant variants\n",
    "json_response = [json.loads(x) for x in reconstructed_response]\n",
    "code_output = [x for x in json_response if x[\"variant\"] == \"CodeOutput\"]\n",
    "print(\"\".join([x[\"content\"][0] for x in code_output])) # This will print the generated code in order.\n",
    "assistant_responses = [x for x in json_response if x[\"variant\"] == \"Assistant\"]\n",
    "print(\"\".join([x[\"content\"] for x in assistant_responses])) # This will print the Assistant's responses in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the 1.8.9 correction of the code_interpreter tool's handling of data:\n",
    "# Does data persist across more than one call?\n",
    "user_input = \"This is a test on a corner case of the code_interpreter tool: variables don't seem to be stored if the code errors before the last line.\\\n",
    "To test this. Please run the following code: \\\"x = 42\\nraise Exception('This is a test exception')\\nprint('Padding for last-line-logic')\\\". After that, run the code \\\"print(x, flush=True)\\\" in a new block.\"\n",
    "url = base_url + '/streamresponse?input=' + user_input + auth_string # leaving out the thread_id spawns a new thread\n",
    "\n",
    "persistant_response = requests.get(url, stream=True) # The response can be streamed or gotten all at once.\n",
    "persistant_response_complete = [] # The stream gets consumed when streamed, we'll store it here.\n",
    "\n",
    "for delta in persistant_response:\n",
    "    print(delta)\n",
    "    persistant_response_complete.append(delta.decode(\"utf-8\"))\n",
    "\n",
    "# Reconstruct the response\n",
    "reconstructed_response = []\n",
    "for i in range(len(persistant_response_complete)):\n",
    "    if persistant_response_complete[i][0] == \"{\":\n",
    "        reconstructed_response.append(persistant_response_complete[i])\n",
    "    else:\n",
    "        reconstructed_response[-1] += persistant_response_complete[i]\n",
    "\n",
    "# Print the reconstructed response\n",
    "for delta in reconstructed_response:\n",
    "    print(delta)\n",
    "\n",
    "# Extract the CodeOutput and Assistant variants\n",
    "json_response = [json.loads(x) for x in reconstructed_response]\n",
    "code = [x for x in json_response if x[\"variant\"] == \"Code\"]\n",
    "code_output = [x for x in json_response if x[\"variant\"] == \"CodeOutput\"]\n",
    "print(\"\".join([x[\"content\"][0] for x in code_output]))\n",
    "print(\"\".join([x[\"content\"][0] for x in code])) # This will print the generated code in order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\".join([x[\"content\"] for x in json_response if x[\"variant\"] == \"Assistant\"])) # This will print the Assistant's responses in order.\n",
    "print()\n",
    "print(\"\\n\".join([x[\"content\"][0] for x in json_response if x[\"variant\"] == \"CodeOutput\"])) # This will print the Code outputs\n",
    "print()\n",
    "print(\"\".join([x[\"content\"][0] for x in json_response if x[\"variant\"] == \"Code\"])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Version 1.8.13, support for o3-mini was added. This is a small test to see if it works.\n",
    "# Note that support for deepseek-r1-70b was also added, but that requires the model to be running locally, which I can't test.\n",
    "# Stream a very simple request to o3-mini instead of the default chatbot.\n",
    "\n",
    "user_input = \"This is a test request for your basic functionality. Please use the code interpreter to print 'Hello World!' and exit.\"\n",
    "url = base_url + '/streamresponse?input=' + user_input + \"&chatbot=o3-mini\" + auth_string # leaving out the thread_id spawns a new thread\n",
    "o3_response = requests.get(url, stream=True) # The response can be streamed or gotten all at once.\n",
    "o3_complete_response = [] # The stream gets consumed when streamed, we'll store it here.\n",
    "\n",
    "for delta in o3_response:\n",
    "    o3_complete_response.append(delta.decode(\"utf-8\"))\n",
    "\n",
    "o3_reconstructed_response = []\n",
    "for i in range(len(o3_complete_response)):\n",
    "    if o3_complete_response[i][0] == \"{\":\n",
    "        o3_reconstructed_response.append(o3_complete_response[i])\n",
    "    else:\n",
    "        o3_reconstructed_response[-1] += o3_complete_response[i]\n",
    "\n",
    "# Print out the response\n",
    "o3_response_json = [json.loads(x) for x in o3_reconstructed_response]\n",
    "print(\"\".join([x[\"content\"] for x in o3_response_json if x[\"variant\"] == \"Assistant\"])) # This will print the Assistant's responses in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in o3_response_json:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During cleanup (version 1.8.15 probably), support for input over headers was added. This is a test to see if it works.\n",
    "# Note that this is not the same as the input over parameters in the streamresponse endpoint, which is still not supported.\n",
    "\n",
    "user_input = \"This is a test request for your basic functionality. Please respond with (200 Ok) and exit.\"\n",
    "url = base_url + '/streamresponse?chatbot=gpt-4o-mini' # leaving out the thread_id spawns a new thread\n",
    "headers = {\n",
    "    'input': user_input,\n",
    "    'Authorization': 'Bearer ' + auth_key, # Only for testing\n",
    "    'X-freva-ConfigPath': 'Cargo.toml' # Dummy value\n",
    "}\n",
    "response = requests.get(url, headers=headers, stream=True) # The response can be streamed or gotten all at once.\n",
    "complete_response = [] # The stream gets consumed when streamed, we'll store it here.\n",
    "for delta in response:\n",
    "    complete_response.append(delta.decode(\"utf-8\"))\n",
    "# Some elements are cut off early if the chunk size is not specified or too small, so to reconstruct, we'll need to append all element that don't start with \"{\" to the previous element.\n",
    "reconstructed_response = []\n",
    "for i in range(len(complete_response)):\n",
    "    if complete_response[i][0] == \"{\":\n",
    "        reconstructed_response.append(complete_response[i])\n",
    "    else:\n",
    "        reconstructed_response[-1] += complete_response[i]\n",
    "for delta in reconstructed_response:\n",
    "    print(delta)\n",
    "# Print out the response\n",
    "response_json = [json.loads(x) for x in reconstructed_response]\n",
    "print(\"\".join([x[\"content\"] for x in response_json if x[\"variant\"] == \"Assistant\"])) # This will print the Assistant's responses in order."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
