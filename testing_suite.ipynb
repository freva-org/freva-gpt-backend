{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pytest\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import ipytest\n",
    "ipytest.autoconfig()\n",
    "\n",
    "base_url = \"http://localhost:8502/api/chatbot\"\n",
    "\n",
    "auth_key = \"qA94VhroHMHFN55inWgfAAkt1WEmzQ4J\" # Only for testing\n",
    "auth_string = \"&auth_key=\" + auth_key + \"&user_id=testing\" # Only for testing\n",
    "# In Version 1.6.1, the freva_config also needs to be set to a specific path. We won't be using this for now.\n",
    "auth_string = auth_string + \"&freva_config=\" + \"Cargo.toml\" # Dummy value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_request(url, stream=False):\n",
    "    return requests.get(base_url + url + auth_string, stream=stream)\n",
    "\n",
    "def get_avail_chatbots():\n",
    "    response = get_request(\"/availablechatbots?\")\n",
    "    print(response.text)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class StreamResult:\n",
    "    chatbot: str | None\n",
    "    raw_response: list = field(default_factory=list)\n",
    "    json_response: list = field(default_factory=list)\n",
    "    code_variants: list = field(default_factory=list)\n",
    "    codeoutput_variants: list = field(default_factory=list)\n",
    "    assistant_variants: list  = field(default_factory=list)\n",
    "    image_variants: list = field(default_factory=list)\n",
    "    server_hint_variants: list  = field(default_factory=list)\n",
    "    thread_id: str | None = None\n",
    "\n",
    "    def extract_variants(self):\n",
    "        if self.json_response:\n",
    "            # The stream can stream multiple Assistant or Code fragments one after the other, in order to get good UX, but that means that multiple fragments that form a single variant can be streamed one after the other.\n",
    "            # So, for convenience, we'll combine consecutive fragments that form a single variant into a single variant, if that variant is Assistant or Code. \n",
    "\n",
    "            running_code = None # None or tuple of (code, code_id) (which is the content of the fragment)\n",
    "            running_assistant = None # None or string (which is the content of the fragment)\n",
    "            for fragment in self.json_response:\n",
    "                variant = fragment[\"variant\"]\n",
    "                content = fragment[\"content\"]\n",
    "\n",
    "                if variant != \"Code\" and running_code:\n",
    "                    self.code_variants.append(running_code)\n",
    "                    running_code = None\n",
    "                if variant != \"Assistant\" and running_assistant:\n",
    "                    self.assistant_variants.append(running_assistant)\n",
    "                    running_assistant = None\n",
    "\n",
    "                if variant == \"Code\":\n",
    "                    if running_code:\n",
    "                        running_code = (running_code[0] + content[0], running_code[1])\n",
    "                    else:\n",
    "                        running_code = (content[0], content[1])\n",
    "                elif variant == \"Assistant\":\n",
    "                    if running_assistant:\n",
    "                        running_assistant = running_assistant + content\n",
    "                    else:\n",
    "                        running_assistant = content\n",
    "                elif variant == \"CodeOutput\":\n",
    "                    self.codeoutput_variants.append(content[0])\n",
    "                elif variant == \"Image\":\n",
    "                    self.image_variants.append(content)\n",
    "                elif variant == \"ServerHint\":\n",
    "                    self.server_hint_variants.append(content)\n",
    "\n",
    "\n",
    "\n",
    "            self.thread_id = json.loads(self.json_response[0][\"content\"])[\"thread_id\"]\n",
    "            print(\"Debug: thread_id: \" + self.thread_id) # Alway print the thread_id for debugging, so that when a test fails, we know which thread_id to look at.\n",
    "\n",
    "def generate_full_respone(user_input, chatbot=None, thread_id=None) -> StreamResult:\n",
    "    inner_url = \"/streamresponse?input=\" + user_input\n",
    "    if chatbot:\n",
    "        inner_url = inner_url + \"&chatbot=\" + chatbot\n",
    "    if thread_id:\n",
    "        inner_url = inner_url + \"&thread_id=\" + thread_id\n",
    "        \n",
    "    # The response is streamed, but we will consume it here and store it\n",
    "    result = StreamResult(chatbot)\n",
    "    response = get_request(inner_url, stream=True)\n",
    "    \n",
    "    unassembled_response = [] # Because the response may not necessary be chunked correctly. We will assemble it here.\n",
    "    for delta in response:\n",
    "        if delta.decode(\"utf-8\")[0] == \"{\":\n",
    "            unassembled_response.append(delta.decode(\"utf-8\"))\n",
    "        else:\n",
    "            unassembled_response[-1] += delta.decode(\"utf-8\")\n",
    "    \n",
    "    # It's assembled now\n",
    "    result.raw_response = unassembled_response\n",
    "    result.json_response = [json.loads(i) for i in unassembled_response]\n",
    "\n",
    "    result.extract_variants()\n",
    "    print(result.json_response) # Print the response for debugging, so that when a test fails, we know what the response was.\n",
    "    return result\n",
    "\n",
    "def get_thread_by_id(thread_id):\n",
    "    reponse = get_request(\"/getthread?thread_id=\" + thread_id)\n",
    "    print(reponse.text)\n",
    "    return reponse.json()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_is_up():\n",
    "    get_request(\"/ping\")\n",
    "    get_request(\"/docs\")\n",
    "    \n",
    "\n",
    "def print_help():\n",
    "    response = get_request(\"/help\") # Same as /ping\n",
    "    print(response.text)\n",
    "\n",
    "def print_docs():\n",
    "    response = get_request(\"/docs\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_available_chatbots():\n",
    "    response = get_avail_chatbots()\n",
    "    assert \"gpt-4o-mini\" in response\n",
    "    assert \"gpt-4o\" in response\n",
    "\n",
    "\n",
    "def get_hello_world_thread_id() -> str:\n",
    "    response = generate_full_respone(\"Please use the code_interpreter tool to run the following code exactly and only once: \\\"print('Hello\\\\nWorld\\\\n!', flush=True)\\\".\", chatbot=\"gpt-4o-mini\")\n",
    "    # Just make sure the code output contains \"Hello World !\"\n",
    "    assert any(\"Hello\\nWorld\\n!\" in i for i in response.codeoutput_variants)\n",
    "    # Now return the thread_id for further testing\n",
    "    return response.thread_id\n",
    "\n",
    "def test_hello_world():\n",
    "    ''' Does the printing of Hello World work? '''\n",
    "    thread_id = get_hello_world_thread_id()\n",
    "    # Now use the thread_id to test the getthread endpoint\n",
    "    hw_thread = get_thread_by_id(thread_id) # Type: list of variants.\n",
    "    temp = StreamResult(None)\n",
    "    temp.json_response = hw_thread\n",
    "    temp.extract_variants()\n",
    "    assert temp.thread_id == thread_id # Just make sure the thread_id is correct\n",
    "    assert any(\"Hello\\nWorld\\n!\" in i for i in temp.codeoutput_variants) # Make sure the code output contains \"Hello World !\"\n",
    "\n",
    "\n",
    "def test_sine_wave(display = False):\n",
    "    ''' Can the code_interpreter tool handle matplotlib and output an image? ''' # Base functionality test\n",
    "    response = generate_full_respone(\"This is a test regarding your capabilities of using the code_interpreter tool and whether it supports matplotlib. Please use the code_interpreter tool to run the following code: \\\"import numpy as np\\nimport matplotlib.pyplot as plt\\nt = np.linspace(-2 * np.pi, 2 * np.pi, 100)\\nsine_wave = np.sin(t)\\nplt.figure(figsize=(10, 5))\\nplt.plot(t, sine_wave, label='Sine Wave')\\nplt.title('Sine Wave from -2π to 2π')\\nplt.xlabel('Angle (radians)')\\nplt.ylabel('Sine value')\\nplt.axhline(0, color='black', linewidth=0.5, linestyle='--')\\nplt.axvline(0, color='black', linewidth=0.5, linestyle='--')\\nplt.grid()\\nplt.legend()\\nplt.show()\\\".\", chatbot=\"gpt-4o-mini\")\n",
    "    # We want to make sure we have generated code, code output and an image. But we want to print the assistant response if it fails.\n",
    "    print(response.assistant_variants)\n",
    "    assert response.code_variants\n",
    "    assert response.codeoutput_variants\n",
    "    assert response.image_variants\n",
    "\n",
    "    if display: # For manual testing, ipytest won't display the image\n",
    "        from IPython.display import display, Image\n",
    "        from base64 import b64decode\n",
    "        for image in response.image_variants:\n",
    "            display(Image(data=b64decode(image), format='png'))\n",
    "\n",
    "\n",
    "def test_persistent_thread_storage():\n",
    "    ''' Does the backend remember the content of a thread? ''' # Base functionality test\n",
    "    response = generate_full_respone(\"Please add 2+2 in the code_interpreter tool.\", chatbot=\"gpt-4o-mini\")\n",
    "    # Now follow up with another request to the same thread_id, to test whether the storage is persistent\n",
    "    response2 = generate_full_respone(\"Now please multiply the result by 3.\", chatbot=\"gpt-4o-mini\", thread_id=response.thread_id)\n",
    "    # The code output should now contain 12\n",
    "    assert any(\"12\" in i for i in response2.codeoutput_variants)\n",
    "\n",
    "\n",
    "def test_persistant_state_storage():\n",
    "    ''' Can the backend refer to the same variable in different tool calls? ''' # Since Version 1.6.3\n",
    "    # Here, we want to test whether the value of a variable is stored between tool calls (not requests)\n",
    "    response = generate_full_respone(\"Please assign the value 42 to the variable x in the code_interpreter tool. After that, call the tool with the code \\\"print(x, flush=True)\\\", without assigning x again. It's a test for the presistance of data.\", chatbot=\"gpt-4o-mini\")\n",
    "    # The code output should now contain 42\n",
    "    assert any(\"42\" in i for i in response.codeoutput_variants)\n",
    "    # Also make sure there are actually two code variants\n",
    "    assert len(response.code_variants) == 2\n",
    "\n",
    "\n",
    "def test_persistant_xarray_storage():\n",
    "    ''' Can the backend refer to the same xarray in different tool calls? ''' # Since Version 1.6.5\n",
    "    reponse = generate_full_respone(\"Please generate a simple xarray dataset in the code_interpreter tool and print out the content. After that, call the tool with the code \\\"print(ds, flush=True)\\\", without generating the dataset again. It's a test for the presistance of data, specifically whether xarray Datasets also work.\", chatbot=\"gpt-4o-mini\")\n",
    "    # The code output should now contain the content of the xarray dataset\n",
    "    assert any(\"xarray.Dataset\" in i for i in reponse.codeoutput_variants)\n",
    "    # Also make sure there are actually two code variants\n",
    "    assert len(reponse.code_variants) == 2\n",
    "\n",
    "\n",
    "def test_qwen_available():\n",
    "    ''' Can the backend use non-OpenAI chatbots, such as Qwen? ''' # Since Version 1.7.1\n",
    "    response = generate_full_respone(\"This is a test request for your basic functionality. Please respond with (200 Ok) and exit.\", chatbot=\"qwen2.5:3b\")\n",
    "    # The assistant output should now contain \"200 Ok\"\n",
    "    assert any(\"(200 Ok)\" in i for i in response.assistant_variants)\n",
    "\n",
    "\n",
    "def test_qwen_code_interpreter():\n",
    "    ''' Can the backend get a code response from Qwen? ''' # Since Version 1.7.1\n",
    "    response = generate_full_respone(\"Please use the code_interpreter tool to run `print(2938429834 * 234987234)`. Make sure to adhere to the JSON format!\", chatbot=\"qwen2.5:3b\")\n",
    "    # The code output should now contain the result of the multiplication\n",
    "    assert any(\"690493498994739156\" in i for i in response.codeoutput_variants)\n",
    "\n",
    "def test_heartbeat():\n",
    "    ''' Can the backend send a heartbeat while a long calculation is running? ''' # Since Version 1.8.1\n",
    "    response = generate_full_respone(\"Please use the code_interpreter tool to run the following code: \\\"import time\\ntime.sleep(7)\\\".\", chatbot=\"gpt-4o-mini\")\n",
    "    # There should now, in total be at least three ServerHint Variants\n",
    "    assert len(response.server_hint_variants) >= 3\n",
    "    # The second Serverhint (first is thread_id) should be JSON containing \"memory\", \"total_memory\", \"cpu_last_minute\", \"process_cpu\" and \"process_memory\"\n",
    "    first_hearbeat = json.loads(response.server_hint_variants[1])\n",
    "    assert \"memory\" in first_hearbeat\n",
    "    assert \"total_memory\" in first_hearbeat\n",
    "    assert \"cpu_last_minute\" in first_hearbeat\n",
    "    assert \"process_cpu\" in first_hearbeat\n",
    "    assert \"process_memory\" in first_hearbeat\n",
    "\n",
    "\n",
    "# TODO: implement 1.8.3 feature of stopping a tool call! (and the 1.8.9 feature that derives from it)\n",
    "\n",
    "\n",
    "def test_syntax_hinting():\n",
    "    ''' Can the backend provide extended hints on syntax errors? ''' # Since Version 1.8.4\n",
    "    response = generate_full_respone(\"Please use the code_interpreter tool to run the following code: \\\"print('Hello World'\\\". This is a test for the improved syntax error reporting. If a hint containing the syntax error is returned, the test is successful.\", chatbot=\"gpt-4o-mini\")\n",
    "    # We can now check the Code Output for the string \"Hint: the error occured on line\", as well as \"SyntaxError\"\n",
    "    assert any(\"Hint: the error occured on line\" in i for i in response.codeoutput_variants)\n",
    "    assert any(\"SyntaxError\" in i for i in response.codeoutput_variants)\n",
    "\n",
    "def test_regression_variable_storage():\n",
    "    ''' Does the backend correctly handle the edge case of variable storage? ''' # Since Version 1.8.9\n",
    "    input = \"This is a test on a corner case of the code_interpreter tool: variables don't seem to be stored if the code errors before the last line.\\\n",
    "To test this. Please run the following code: \\\"x = 42\\nraise Exception('This is a test exception')\\nprint('Padding for last-line-logic')\\\",\"\n",
    "    response = generate_full_respone(input, chatbot=\"gpt-4o-mini\")\n",
    "    # The code output should now contain the exception message\n",
    "    assert any(\"This is a test exception\" in i for i in response.codeoutput_variants)\n",
    "\n",
    "    # Now make sure the variable x is still stored\n",
    "    response2 = generate_full_respone(\"Now print the value of x without assigning it again.\", chatbot=\"gpt-4o-mini\", thread_id=response.thread_id)\n",
    "    # The code output should now contain 42\n",
    "    assert any(\"42\" in i for i in response2.codeoutput_variants)\n",
    "\n",
    "\n",
    "def test_o3_mini_available():\n",
    "    ''' Can the backend use the O3-Mini chatbot, including for code_interpreter tool calls? ''' # Since Version 1.8.13\n",
    "    response = generate_full_respone(\"This is a test request for your basic functionality. Please use the code_interpreter tool to run `print('Hello World')` and exit.\", chatbot=\"o3-mini\")\n",
    "    # The Code Output should now contain \"Hello World\"\n",
    "    assert any(\"Hello World!\" in i for i in response.codeoutput_variants)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\n",
      "============================================= FAILURES =============================================\n",
      "\u001b[31m\u001b[1m____________________________________ test_qwen_code_interpreter ____________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_qwen_code_interpreter\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m''' Can the backend get a code response from Qwen? '''\u001b[39;49;00m \u001b[90m# Since Version 1.7.1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        response = generate_full_respone(\u001b[33m\"\u001b[39;49;00m\u001b[33mPlease use the code_interpreter tool to run `print(2938429834 * 234987234)`. Make sure to adhere to the JSON format!\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, chatbot=\u001b[33m\"\u001b[39;49;00m\u001b[33mqwen2.5:3b\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# The code output should now contain the result of the multiplication\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[96many\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m690493498994739156\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m i \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m response.codeoutput_variants)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where False = any(<generator object test_qwen_code_interpreter.<locals>.<genexpr> at 0x10ec15be0>)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/8k/16sqpxjs1gq7mwfjfywl9wlw0000gn/T/ipykernel_85723/1206266436.py\u001b[0m:81: AssertionError\n",
      "--------------------------------------- Captured stdout call ---------------------------------------\n",
      "Debug: thread_id: 722LtzeqqsPLKGSB7XQpuf0c2zjiUyCs\n",
      "[{'variant': 'ServerHint', 'content': '{\"thread_id\": \"722LtzeqqsPLKGSB7XQpuf0c2zjiUyCs\"}'}, {'variant': 'Code', 'content': ['{\"code\":\"print(2938429834 * 234987234)\"}', 'call_s9hmma7h']}, {'variant': 'Assistant', 'content': ''}, {'variant': 'OpenAIError', 'content': 'No response found.'}, {'variant': 'StreamEnd', 'content': 'Ollama Stream ended'}]\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_58b73c958e0b46cdb5c4ba9a38dbf1d0.py::\u001b[1mtest_qwen_code_interpreter\u001b[0m - assert False\n",
      "\u001b[31m!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! stopping after 1 failures !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\u001b[0m\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m8 passed\u001b[0m\u001b[31m in 83.53s (0:01:23)\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.TESTS_FAILED: 1>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipytest.run(\"-lf\") # Fail on first error\n",
    "# TODO: is there a way to only run failed tests again?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
