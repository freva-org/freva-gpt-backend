{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pytest\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import ipytest\n",
    "ipytest.autoconfig()\n",
    "\n",
    "base_url = \"http://localhost:8502/api/chatbot\"\n",
    "\n",
    "auth_key = \"qA94VhroHMHFN55inWgfAAkt1WEmzQ4J\" # Only for testing\n",
    "auth_string = \"&auth_key=\" + auth_key + \"&user_id=testing\" # Only for testing\n",
    "# In Version 1.6.1, the freva_config also needs to be set to a specific path. We won't be using this for now.\n",
    "auth_string = auth_string + \"&freva_config=\" + \"Cargo.toml\" # Dummy value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_request(url, stream=False):\n",
    "    return requests.get(base_url + url + auth_string, stream=stream)\n",
    "\n",
    "def get_avail_chatbots():\n",
    "    return get_request(\"/availablechatbots\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class StreamResult:\n",
    "    chatbot: str | None\n",
    "    raw_response: list\n",
    "    json_response: list\n",
    "    code_variants: list | None = None\n",
    "    codeoutput_variants: list | None = None\n",
    "    assistant_variants: list | None = None\n",
    "    image_variants: list | None = None\n",
    "    server_hint_variants: list | None = None\n",
    "    thread_id: str | None = None\n",
    "\n",
    "    def extract_variants(self):\n",
    "        if self.json_response:\n",
    "            self.code_variants = [i[\"content\"][0] for i in self.json_response if i[\"variant\"] == \"Code\"]\n",
    "            self.codeoutput_variants = [i[\"content\"][0] for i in self.json_response if i[\"variant\"] == \"CodeOutput\"]\n",
    "            self.assistant_variants = [i[\"content\"] for i in self.json_response if i[\"variant\"] == \"Assistant\"]\n",
    "            self.image_variants = [i[\"content\"] for i in self.json_response if i[\"variant\"] == \"Image\"]\n",
    "            self.server_hint_variants = [i[\"content\"] for i in self.json_response if i[\"variant\"] == \"ServerHint\"]\n",
    "            self.thread_id = json.loads(self.json_response[0][\"content\"])[\"thread_id\"]\n",
    "\n",
    "def generate_full_respone(user_input, chatbot=None, thread_id=None) -> StreamResult:\n",
    "    inner_url = \"/streamresponse?input=\" + user_input\n",
    "    if chatbot:\n",
    "        inner_url = inner_url + \"&chatbot=\" + chatbot\n",
    "    if thread_id:\n",
    "        inner_url = inner_url + \"&thread_id=\" + thread_id\n",
    "        \n",
    "    # The response is streamed, but we will consume it here and store it\n",
    "    result = StreamResult(chatbot)\n",
    "    response = get_request(inner_url, stream=True)\n",
    "    \n",
    "    unassembled_response = [] # Because the response may not necessary be chunked correctly. We will assemble it here.\n",
    "    for delta in response:\n",
    "        if delta.decode(\"utf-8\")[0] == \"{\":\n",
    "            unassembled_response.append(delta.decode(\"utf-8\"))\n",
    "        else:\n",
    "            unassembled_response[-1] += delta.decode(\"utf-8\")\n",
    "    \n",
    "    # It's assembled now\n",
    "    result.raw_response = unassembled_response\n",
    "    result.json_response = [json.loads(i) for i in unassembled_response]\n",
    "\n",
    "    result.extract_variants()\n",
    "    return result\n",
    "\n",
    "def get_thread_by_id(thread_id):\n",
    "    return get_request(\"/thread?thread_id=\" + thread_id).json()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_is_up():\n",
    "    get_request(\"/ping\")\n",
    "    get_request(\"/docs\")\n",
    "    \n",
    "\n",
    "def print_help():\n",
    "    response = get_request(\"/help\") # Same as /ping\n",
    "    print(response.text)\n",
    "\n",
    "def print_docs():\n",
    "    response = get_request(\"/docs\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_available_chatbots():\n",
    "    response = get_avail_chatbots()\n",
    "    assert \"gpt-4o-mini\" in response\n",
    "    assert \"gpt-4o\" in response\n",
    "\n",
    "\n",
    "def get_hello_world_thread_id() -> str:\n",
    "    response = generate_full_respone(\"Please use the code_interpreter tool to run the following code exactly and only once: \\\"print('Hello\\\\nWorld\\\\n!', flush=True)\\\".\", chatbot=\"gpt-4o-mini\")\n",
    "    # Just make sure the code output contains \"Hello World !\"\n",
    "    assert any(\"Hello\\nWorld\\n!\" in i for i in response.codeoutput_variants)\n",
    "    # Now return the thread_id for further testing\n",
    "    return response.thread_id\n",
    "\n",
    "def test_hello_world():\n",
    "    ''' Does the printing of Hello World work? '''\n",
    "    thread_id = get_hello_world_thread_id()\n",
    "    # Now use the thread_id to test the getthread endpoint\n",
    "    hw_thread = get_thread_by_id(thread_id)\n",
    "    assert hw_thread[\"thread_id\"] == thread_id # Just make sure the thread_id is correct\n",
    "    assert any(\"Hello\\nWorld\\n!\" in i for i in hw_thread[\"codeoutput_variants\"]) # Make sure the code output contains \"Hello World !\"\n",
    "\n",
    "\n",
    "def test_sine_wave(display = False):\n",
    "    ''' Can the code_interpreter tool handle matplotlib and output an image? ''' # Base functionality test\n",
    "    response = generate_full_respone(\"This is a test regarding your capabilities of using the code_interpreter tool and whether it supports matplotlib. Please use the code_interpreter tool to run the following code: \\\"import numpy as np\\nimport matplotlib.pyplot as plt\\nt = np.linspace(-2 * np.pi, 2 * np.pi, 100)\\nsine_wave = np.sin(t)\\nplt.figure(figsize=(10, 5))\\nplt.plot(t, sine_wave, label='Sine Wave')\\nplt.title('Sine Wave from -2π to 2π')\\nplt.xlabel('Angle (radians)')\\nplt.ylabel('Sine value')\\nplt.axhline(0, color='black', linewidth=0.5, linestyle='--')\\nplt.axvline(0, color='black', linewidth=0.5, linestyle='--')\\nplt.grid()\\nplt.legend()\\nplt.show()\\\".\", chatbot=\"gpt-4o-mini\")\n",
    "    # We want to make sure we have generated code, code output and an image. But we want to print the assistant response if it fails.\n",
    "    print(response.assistant_variants)\n",
    "    assert response.code_variants\n",
    "    assert response.codeoutput_variants\n",
    "    assert response.image_variants\n",
    "\n",
    "    if display: # For manual testing, ipytest won't display the image\n",
    "        from IPython.display import display, Image\n",
    "        from base64 import b64decode\n",
    "        for image in response.image_variants:\n",
    "            display(Image(data=b64decode(image), format='png'))\n",
    "\n",
    "\n",
    "def test_persistent_thread_storage():\n",
    "    ''' Does the backend remember the content of a thread? ''' # Base functionality test\n",
    "    response = generate_full_respone(\"Please add 2+2 in the code_interpreter tool.\", chatbot=\"gpt-4o-mini\")\n",
    "    # Now follow up with another request to the same thread_id, to test whether the storage is persistent\n",
    "    response2 = generate_full_respone(\"Now please multiply the result by 3.\", chatbot=\"gpt-4o-mini\", thread_id=response.thread_id)\n",
    "    # The code output should now contain 12\n",
    "    assert any(\"12\" in i for i in response2.codeoutput_variants)\n",
    "\n",
    "\n",
    "def test_persistant_state_storage():\n",
    "    ''' Can the backend refer to the same variable in different tool calls? ''' # Since Version 1.6.3\n",
    "    # Here, we want to test whether the value of a variable is stored between tool calls (not requests)\n",
    "    response = generate_full_respone(\"Please assign the value 42 to the variable x in the code_interpreter tool. After that, call the tool with the code \\\"print(x, flush=True)\\\", without assigning x again. It's a test for the presistance of data.\", chatbot=\"gpt-4o-mini\")\n",
    "    # The code output should now contain 42\n",
    "    assert any(\"42\" in i for i in response.codeoutput_variants)\n",
    "    # Also make sure there are actually two code variants\n",
    "    assert len(response.code_variants) == 2\n",
    "\n",
    "\n",
    "def test_persistant_xarray_storage():\n",
    "    ''' Can the backend refer to the same xarray in different tool calls? ''' # Since Version 1.6.5\n",
    "    reponse = generate_full_respone(\"Please generate a simple xarray dataset in the code_interpreter tool and print out the content. After that, call the tool with the code \\\"print(ds, flush=True)\\\", without generating the dataset again. It's a test for the presistance of data, specifically whether xarray Datasets also work.\", chatbot=\"gpt-4o-mini\")\n",
    "    # The code output should now contain the content of the xarray dataset\n",
    "    assert any(\"xarray.Dataset\" in i for i in reponse.codeoutput_variants)\n",
    "    # Also make sure there are actually two code variants\n",
    "    assert len(reponse.code_variants) == 2\n",
    "\n",
    "\n",
    "def test_qwen_available():\n",
    "    ''' Can the backend use non-OpenAI chatbots, such as Qwen? ''' # Since Version 1.7.1\n",
    "    response = generate_full_respone(\"This is a test request for your basic functionality. Please respond with (200 Ok) and exit.\", chatbot=\"qwen2.5:3b\")\n",
    "    # The assistant output should now contain \"200 Ok\"\n",
    "    assert any(\"(200 Ok)\" in i for i in response.assistant_variants)\n",
    "\n",
    "\n",
    "def test_qwen_code_interpreter():\n",
    "    ''' Can the backend get a code response from Qwen? ''' # Since Version 1.7.1\n",
    "    response = generate_full_respone(\"Please use the code_interpreter tool to run `print(2938429834 * 234987234)`. Make sure to adhere to the JSON format!\", chatbot=\"qwen2.5:3b\")\n",
    "    # The code output should now contain the result of the multiplication\n",
    "    assert any(\"690493498994739156\" in i for i in response.codeoutput_variants)\n",
    "\n",
    "def test_heartbeat():\n",
    "    ''' Can the backend send a heartbeat while a long calculation is running? ''' # Since Version 1.8.1\n",
    "    response = generate_full_respone(\"Please use the code_interpreter tool to run the following code: \\\"import time\\ntime.sleep(7)\\\".\", chatbot=\"gpt-4o-mini\")\n",
    "    # There should now, in total be at least three ServerHint Variants\n",
    "    assert len(response.server_hint_variants) >= 3\n",
    "    # The second Serverhint (first is thread_id) should be JSON containing \"memory\", \"total_memory\", \"cpu_last_minute\", \"process_cpu\" and \"process_memory\"\n",
    "    first_hearbeat = json.loads(response.server_hint_variants[1])\n",
    "    assert \"memory\" in first_hearbeat\n",
    "    assert \"total_memory\" in first_hearbeat\n",
    "    assert \"cpu_last_minute\" in first_hearbeat\n",
    "    assert \"process_cpu\" in first_hearbeat\n",
    "    assert \"process_memory\" in first_hearbeat\n",
    "\n",
    "\n",
    "# TODO: implement 1.8.3 feature of stopping a tool call! (and the 1.8.9 feature that derives from it)\n",
    "\n",
    "\n",
    "def test_syntax_hinting():\n",
    "    ''' Can the backend provide extended hints on syntax errors? ''' # Since Version 1.8.4\n",
    "    response = generate_full_respone(\"Please use the code_interpreter tool to run the following code: \\\"print('Hello World'\\\". This is a test for the improved syntax error reporting. If a hint containing the syntax error is returned, the test is successful.\", chatbot=\"gpt-4o-mini\")\n",
    "    # We can now check the Code Output for the string \"Hint: the error occured on line\", as well as \"SyntaxError\"\n",
    "    assert any(\"Hint: the error occured on line\" in i for i in response.codeoutput_variants)\n",
    "    assert any(\"SyntaxError\" in i for i in response.codeoutput_variants)\n",
    "\n",
    "def test_regression_variable_storage():\n",
    "    ''' Does the backend correctly handle the edge case of variable storage? ''' # Since Version 1.8.9\n",
    "    input = \"This is a test on a corner case of the code_interpreter tool: variables don't seem to be stored if the code errors before the last line.\\\n",
    "To test this. Please run the following code: \\\"x = 42\\nraise Exception('This is a test exception')\\nprint('Padding for last-line-logic')\\\",\"\n",
    "    response = generate_full_respone(input, chatbot=\"gpt-4o-mini\")\n",
    "    # The code output should now contain the exception message\n",
    "    assert any(\"This is a test exception\" in i for i in response.codeoutput_variants)\n",
    "\n",
    "    # Now make sure the variable x is still stored\n",
    "    response2 = generate_full_respone(\"Now print the value of x without assigning it again.\", chatbot=\"gpt-4o-mini\", thread_id=response.thread_id)\n",
    "    # The code output should now contain 42\n",
    "    assert any(\"42\" in i for i in response2.codeoutput_variants)\n",
    "\n",
    "\n",
    "def test_o3_mini_available():\n",
    "    ''' Can the backend use the O3-Mini chatbot, including for code_interpreter tool calls? ''' # Since Version 1.8.13\n",
    "    response = generate_full_respone(\"This is a test request for your basic functionality. Please use the code_interpreter tool to run `print('Hello World')` and exit.\", chatbot=\"o3-mini\")\n",
    "    # The Code Output should now contain \"Hello World\"\n",
    "    assert any(\"Hello World!\" in i for i in response.codeoutput_variants)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipytest.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
