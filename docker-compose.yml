services:
  freva-gpt2-backend:
    image: freva-gpt2-backend-${INSTANCE_NAME:?Instance name must be set in the .env file to differentiate between instances}
    build:
      context: .
      dockerfile: dockerfiles/chatbot-backend/Dockerfile
      cgroup_manager: cgroupfs # Note: This requires Podman, docker is allergic to this option.
    hostname: freva-gpt2-backend-instance-${INSTANCE_NAME}
    ports:
      - "${TARGET_PORT:-8502}:${BACKEND_PORT:-8502}" # If either is unset, the default value will be used.
    volumes:
      - ./resources:/app/resources
      - /work:/work:ro
      - /container/da/genai_data:/container/da/genai_data
      - /container/da/storage-b380001/freva-gpt-backend-links-${INSTANCE_NAME}/logs:/app/logs
      - /container/da/storage-b380001/freva-gpt-backend-links-${INSTANCE_NAME}/threads:/app/threads
      - /container/da/storage-b380001/freva-gpt-backend-links-${INSTANCE_NAME}/target:/app/target
      - /container/da/storage-b380001/freva-gpt-backend-links-${INSTANCE_NAME}/python_pickles:/app/python_pickles
      - /home/b/b380001/freva-gpt2-backend/testdata:/data/inputFiles # Deprecated, once used for test data before freva support was added. Now used for debugging.
      - /scratch/b/b380001/freva-gpt/python_pickles:/app/python_pickles
      - /scratch/b/b380001/freva-gpt/rw_dir/:/app/rw_dir # All instances share the rw_dir for accessing the data the LLM generated.
    networks:
      - freva-gpt
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "-f",
          "http://localhost:${TARGET_PORT:-8502}/api/chatbot/help",
        ]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
  litellm:
    image: ghcr.io/berriai/litellm:main-stable
    hostname: litellm-instance-${INSTANCE_NAME}
    volumes:
      - ./litellm_config.yaml:/app/config.yaml # Mount the local litellm_config.yaml file to the container
    command:
      - "--config=/app/config.yaml"
    # ports:
    #   - 4000:4000
    # DO NOT open the litellm port to the outside; instead, configure the backend to connect to the container directly. See the example config.
    env_file:
      - .env # Load local .env file
    networks:
      - freva-gpt
    healthcheck:
      test: "curl http://localhost:4000/health/readiness"
  rag-server:
    image: localhost/rag-server-${INSTANCE_NAME}
    hostname: rag-server-instance-${INSTANCE_NAME}
    platform: linux/amd64
    build:
      context: .
      dockerfile: dockerfiles/rag-server/Dockerfile
    env_file: .env
    environment:
      MCP_HOST: 0.0.0.0
      MCP_PORT: 8050
    ports:
      - "8050:8050"
    volumes:
      - ./resources:/app/resources:ro
    networks:
      - freva-gpt
  code-server:
    platform: linux/amd64
    image: localhost/code-interpreter-server-${INSTANCE_NAME}
    build:
      context: .
      dockerfile: docker/code-interpreter-server/Dockerfile
    env_file: .env
    environment:
      MCP_HOST: 0.0.0.0
      MCP_PORT: 8051
    ports:
      - "8051:8051"
    volumes:
      - /work:/work:ro
    networks:
      - freva-gpt
  ollama:
    image: ollama/ollama
    hostname: ollama-instance-${INSTANCE_NAME}
    volumes:
      - /container/da/freva-gpt-ollama-LLMs/.ollama:/root/.ollama
    networks:
      - freva-gpt
    healthcheck:
      test: "bash -c 'cat < /dev/null > /dev/tcp/localhost/11434'"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

networks:
  freva-gpt:
    driver: bridge
# The subnet is not set to avoid conflicts with other potentially running networks on the same machine.
# Note: Named networks by default aren't shared between different docker-compose projects, even if they have the same name, as long as they are in separate directories.
