services:
  freva-gpt2-backend:
    image: freva-gpt2-backend-${INSTANCE_NAME}
    build:
      context: .
      dockerfile: dockerfiles/chatbot-backend/Dockerfile
      cgroup_manager: cgroupfs
      tags:
        - freva-gpt2-backend:${INSTANCE_NAME}
    hostname: freva-gpt2-backend-instance-${INSTANCE_NAME}
    ports:
      - "8503:8502"
    volumes:
      - /work:/work:ro
      - /container/da/storage-b380001/freva-gpt-backend-links-${INSTANCE_NAME}/logs:/app/logs
      - /container/da/storage-b380001/freva-gpt-backend-links-${INSTANCE_NAME}/threads:/app/threads
      - /container/da/storage-b380001/freva-gpt-backend-links-${INSTANCE_NAME}/target:/app/target
      - /container/da/storage-b380001/freva-gpt-backend-links-${INSTANCE_NAME}/python_pickles:/app/python_pickles
      - /home/b/b380001/freva-gpt2-backend/testdata:/data/inputFiles # Deprecated, once used for test data before freva support was added. Now used for debugging.
    networks:
      - freva-gpt2-backend
  litellm:
    image: ghcr.io/berriai/litellm:main-stable
    hostname: litellm-instance-${INSTANCE_NAME}
    volumes:
      - ./litellm_config.yaml:/app/config.yaml # Mount the local litellm_config.yaml file to the container
    command:
      - "--config=/app/config.yaml"
    ports:
      - "4000:4000" # Map the container port to the host, change the host port if necessary
    env_file:
      - .env # Load local .env file
    networks:
      - freva-gpt2-backend
  ollama:
    image: ollama/ollama
    container_name: ollama-instance
    hostname: ollama-instance-${INSTANCE_NAME}
    volumes:
      - /container/da/freva-gpt-ollama-LLMs/.ollama:/root/.ollama
    # ports:
    # - "11434:11434" # Map the container port to the host, change the host port if necessary
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - freva-gpt2-backend

networks:
  freva-gpt2-backend:
    driver: bridge
